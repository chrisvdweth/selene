{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a927d8",
   "metadata": {},
   "source": [
    "### Auto-Generated Code Cells\n",
    "\n",
    "The following code cells contain code from external .py files that has been automatically added to remove these dependencies. This allows you to run this version of the SELENE notebook on Cloud platforms such as Google Colab. Run all following code cells before starting with the actual notebook content. \n",
    "\n",
    "Most Cloud platforms such as Google Colab have a wide range of Python packages preinstalled. However, if any import statements in the following code cells throws an error because of a missing package, you can create and run a new code cell to install the missing package using the `pip` command as shown here:\n",
    "```\n",
    "!pip install <package-name>\n",
    "```\n",
    "\n",
    "You can use the code cell below for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install <package-name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e73bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inline content for module src.utils.data.files  ==\n",
    "\n",
    "# === Inlined imports ===\n",
    "import bz2\n",
    "import os, re, yaml\n",
    "import requests\n",
    "import zipfile, tarfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Embedded configs ===\n",
    "CONFIG = yaml.safe_load(\"\"\"urls:\n",
    "  downloads:\n",
    "    notebooks: \"https://github.com/chrisvdweth/selene/tree/master/notebooks\"\n",
    "    datasets: \"https://chrisvdw.net/projects/selene/downloads/datasets/\"\n",
    "    models: \"https://chrisvdw.net/projects/selene/downloads/models/\"\n",
    "    \"\"\")\n",
    "\n",
    "# === Inlined definitions ===\n",
    "def download_dataset(dataset_path, base_url=None, download_path=None, overwrite=False, ignore_html=True):\n",
    "    if base_url is None:\n",
    "        base_url = CONFIG['urls']['downloads']['datasets']\n",
    "    if download_path is None:\n",
    "        download_path = 'data/datasets/' + '/'.join(dataset_path.split('/')[0:-1]) + '/'\n",
    "    url = base_url + dataset_path\n",
    "    return download_file(url, download_path=download_path, overwrite=overwrite, ignore_html=ignore_html)\n",
    "\n",
    "def download_file(url, download_path, overwrite=False, ignore_html=False):\n",
    "    file_name = url.split('/')[-1]\n",
    "    create_folder(download_path)\n",
    "    file_path = download_path + file_name\n",
    "    if os.path.isfile(file_path) == True and overwrite is not True:\n",
    "        print(f\"File '{file_path}' already exists (use 'overwrite=True' to overwrite it).\")\n",
    "        return (file_path, download_path)\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            if ignore_html is True and is_html_file(data) is True:\n",
    "                print('Error downloading file (expected data file, got HTML file)')\n",
    "                return (None, None)\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print('Error downloading file (source does not exist)')\n",
    "        return (None, None)\n",
    "    return (file_path, download_path)\n",
    "\n",
    "def is_html_file(content):\n",
    "    content = content.decode('utf-8', 'ignore').strip().lower()\n",
    "    if content.startswith('<!doctype html') is True:\n",
    "        return True\n",
    "    elif content.startswith('<html') is True:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def create_folder(folder_name, exist_ok=True):\n",
    "    try:\n",
    "        os.makedirs(folder_name, exist_ok=exist_ok)\n",
    "        return folder_name\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9b630-db3e-49a7-8af1-f50c95056120",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/logo/selene-logo-640.png\" style=\"max-height:75px;\" alt=\"SELENE Logo\" />\n",
    "\n",
    "**Disclaimer:** This Jupyter Notebook contains content generated with the assistance of AI. While every effort has been made to review and validate the outputs, users should independently verify critical information before relying on it. The SELENE notebook repository is constantly evolving. We recommend downloading or pulling the latest version of this notebook from Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab079cdb-482b-40cf-984a-ae8d31d9d52f",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees are one of the most intuitive and widely used machine learning algorithms for both classification and regression tasks. At their core, decision trees represent a flowchart-like structure, where data is split recursively based on certain conditions until a final decision or prediction is made. Each internal node of the tree corresponds to a decision or test on a feature, each branch represents the outcome of the test, and each leaf node signifies a final prediction or category. Their hierarchical nature makes decision trees easy to visualize and interpret, making them highly suitable for both beginners in machine learning and professionals seeking explainable AI models. The versatility of decision trees allows them to excel in a range of applications, including but not limited to, medical diagnosis, customer segmentation, fraud detection, and predictive maintenance. For example, a decision tree might help a doctor determine whether a patient has a particular disease by analyzing their symptoms step-by-step. Similarly, businesses use decision trees to classify customer behavior and tailor marketing strategies. Their ability to handle both categorical and continuous data makes them a robust choice for solving real-world problems.\n",
    "\n",
    "Learning about decision trees is crucial for several reasons. First, they provide a foundational understanding of how data-driven decisions are made, which is essential for grasping more advanced algorithms like Random Forests and Gradient Boosting Machines. Second, decision trees are inherently interpretable, making them ideal for scenarios where transparency is necessary, such as in healthcare or legal domains. Finally, their simplicity and efficiency in handling both small and large datasets make them a fundamental tool in any data scientist's toolbox. In addition, decision trees play a pivotal role in ensemble learning methods, where multiple decision trees are combined to create more robust and accurate models. Algorithms like Random Forests and XGBoost rely on decision trees as their building blocks, showcasing their importance even in advanced machine learning techniques. By understanding the principles of decision trees, one can better comprehend and implement these powerful ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75587e-eaef-425d-966e-79e8bb10305c",
   "metadata": {},
   "source": [
    "### Setting up the Notebook\n",
    "\n",
    "#### Make Required Imports\n",
    "\n",
    "This notebook requires the import of different Python packages but also additional Python modules that are part of the repository. If a package is missing, use your preferred package manager (e.g., [conda](https://anaconda.org/anaconda/conda) or [pip](https://pypi.org/project/pip/)) to install it. If the code cell below runs with any errors, all required packages and modules have successfully been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52800b0c-d2a8-492c-918b-487015dacfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287960b0-459c-4000-b318-b117100906f4",
   "metadata": {},
   "source": [
    "#### Download Required Data\n",
    "\n",
    "Some code examples in this notebook use data that first need to be downloaded by running the code cell below. If this code cell throws any error, please check the configuration file `config.yaml` if the URL for downloading datasets is up to date and matches the one on Github. If not, simply download or pull the latest version from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d011f6-db89-4a87-82c1-d04ec0edcf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/datasets/tabular/classification/example-credit-default-data.csv' already exists (use 'overwrite=True' to overwrite it).\n"
     ]
    }
   ],
   "source": [
    "bank_data ,_ = download_dataset(\"tabular/classification/example-credit-default-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14f860-266a-46cc-8f2a-4b8909810ab2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a82978-d95f-4a70-85bc-5147485e6ed1",
   "metadata": {},
   "source": [
    "## Basic Idea\n",
    "\n",
    "### Example Application\n",
    "\n",
    "Banks give out credit as a fundamental part of their business model to earn profits and facilitate economic activity. By extending credit in the form of loans, mortgages, or credit cards, banks generate revenue through the interest charged on the borrowed amount. This interest is a primary source of income for banks, enabling them to cover operational costs, pay interest to depositors, and provide returns to shareholders. Offering credit also helps banks attract customers and build long-term relationships, which can lead to cross-selling of other financial products like insurance, investment services, or savings accounts. Issues arise when a bank customer \"defaults\" on their credit, which means they have failed to meet the repayment obligations outlined in their credit agreement. Consequences for the bank include not only financial losses but also operation costs (costs are associated with attempting to collect the debt or writing it off), and increased risk reserves (the bank must allocate more capital to cover potential future losses, which affects its profitability and lending capacity).\n",
    "\n",
    "Therefore, banks try to predict whether a customer will default on their credit to manage risks effectively and ensure financial stability. Predicting defaults allows banks to identify high-risk customers in advance and take preventive measures, such as adjusting credit limits, requiring collateral, or offering stricter terms for repayment. This risk assessment ensures that the bank minimizes potential losses while continuing to lend profitably. Moreover, predicting credit defaults is crucial for compliance with regulatory requirements. Financial institutions are often required to maintain adequate capital reserves to cover potential losses from defaults. By accurately predicting which customers are likely to default, banks can allocate these reserves more effectively and ensure they meet regulatory standards. This not only helps maintain trust in the banking system but also contributes to overall economic stability. Predicting defaults also enhances customer management, as banks can proactively engage with at-risk customers to explore solutions like debt restructuring or financial counseling, benefiting both parties.\n",
    "\n",
    "In the following, we play the bank and use a very simple example dataset of customers' information and whether they defaulted on their credit or not. The dataset contains the credit application record for only nine customers, with each customer describe by four attributes: *Age*, *Education* (Bachelor, Masters, or PhD), *Martial_Status* (Single or Married), and *Annual_Income*. The attribute *Credit_Default* (Yes or No) indicates if the customer defaulted on their credit or not. Let's load the data from the file into a pandas DataFrame and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d39ef12-acae-49f1-b503-d1ae41e7dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Credit_Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Single</td>\n",
       "      <td>75000</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Married</td>\n",
       "      <td>50000</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Single</td>\n",
       "      <td>70000</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Single</td>\n",
       "      <td>95000</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Single</td>\n",
       "      <td>40000</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Married</td>\n",
       "      <td>85000</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Single</td>\n",
       "      <td>60000</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>60000</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Married</td>\n",
       "      <td>60000</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Age Education Marital_Status  Annual_Income Credit_Default\n",
       "0   1   23   Masters         Single          75000             NO\n",
       "1   2   35  Bachelor        Married          50000            YES\n",
       "2   3   26   Masters         Single          70000             NO\n",
       "3   4   41       PhD         Single          95000             NO\n",
       "4   5   18  Bachelor         Single          40000            YES\n",
       "5   6   55   Masters        Married          85000            YES\n",
       "6   7   30  Bachelor         Single          60000            YES\n",
       "7   8   35       PhD        Married          60000             NO\n",
       "8   9   28       PhD        Married          60000             NO"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bank = pd.read_csv(bank_data)\n",
    "\n",
    "df_bank.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcc91a-01ab-45ca-9f61-58380ba8a1b1",
   "metadata": {},
   "source": [
    "For example, the customer with ID 1 was 23 years of age, had a Master's degree, was single, had an annual income of $75,000, and did not default on their credit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c0e1e-30b1-496c-bfaf-e4ee691bc411",
   "metadata": {},
   "source": [
    "#### Rule-Based Classification\n",
    "\n",
    "Now assume we want to find some rules that could tell us if new customers are likely to default on their credit based on this historical dataset. If the result would say *\"YES\"* then we might consider not approving their credit (or adjusting credit limits, requiring collateral, or offering stricter terms for repayment). Just by looking at this data set, you might notice some patterns, for example:\n",
    "\n",
    "* All customers that were single and earned more than $S65k did not default on their credit\n",
    "* All customers with a Master's degree and older than 50 did default on their credit\n",
    "* ...\n",
    "\n",
    "The goal is to find a rule set that correctly captures all customers in our dataset. If we would take some time and try different combinations of conditions, we might come up with the following rule set in pseudo code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66e35b-b76f-461a-b3b0-56acf38ffc91",
   "metadata": {},
   "source": [
    "```\n",
    "IF Marital_Status IS \"Single\" THEN\n",
    "    IF Annual_Income < 65000 THEN\n",
    "        RETURN \"YES\"\n",
    "    ELSE\n",
    "        RETURN \"NO\"\n",
    "    END IF\n",
    "ELSE\n",
    "    IF Education IS \"PhD\" THEN\n",
    "        RETURN \"NO\"\n",
    "    ELSE\n",
    "        RETURN \"YES\"\n",
    "    END IF\n",
    "END IF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa139b-6124-4336-9301-dd87f4349974",
   "metadata": {},
   "source": [
    "You can try each customer in the dataset to convince yourself that the rule set indeed matches a customer to the recorded output. In fact, we can quickly implement this rule set by transforming the previous pseudo code into a Python method that takes the four relevant attributes as input and returns either *\"YES\"* or *\"NO\"*  for the predicted *Credit_Default* attribute. Notice that we call the method `predict_default_v1()` since we will later implement a second predict method accomplishing the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c51492-5857-4a5f-88e8-fed22322e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_default_v1(age, education, marital_status, annual_income):\n",
    "    if marital_status == \"Single\":\n",
    "        if annual_income < 65000:\n",
    "            return \"YES\"\n",
    "        else:\n",
    "            return \"NO\"\n",
    "    else:\n",
    "        if education == \"PhD\":\n",
    "            return \"NO\"\n",
    "        else:\n",
    "            return \"YES\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603ffae-5686-4b15-a2ec-85766c336c4d",
   "metadata": {},
   "source": [
    "We can now iterate over all entries in our customer dataset to predict if the customer would default on their credit using the methods `predict_default_v1()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0b4c53-67c0-43cf-bf6b-004fbd3d0d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ID 1] NO\n",
      "[ID 2] YES\n",
      "[ID 3] NO\n",
      "[ID 4] NO\n",
      "[ID 5] YES\n",
      "[ID 6] YES\n",
      "[ID 7] YES\n",
      "[ID 8] NO\n",
      "[ID 9] NO\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df_bank.iterrows():\n",
    "    prediction = predict_default_v1(row[\"Age\"], row[\"Education\"], row[\"Marital_Status\"], row[\"Annual_Income\"])\n",
    "    print(f\"[ID {idx+1}] {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743c58f-0895-42de-9588-486304cdbc44",
   "metadata": {},
   "source": [
    "The predicted results should match the values in the *Credit_Default* column in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0491e-c950-4531-afc8-2c6e4e3e582a",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "If you look at the pseudo or Python code, you will notice that the rule set is implemented using nested if...else statements with each condition introducing a conditional branch. Such a branching structure can be graphically represented by a tree, giving us our basic idea of a Decision Tree. The figure below shows the Decision Tree reflecting the rule set we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a3cc2-0371-4982-8550-3e86eb022eef",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-01.png\" style=\"margin:auto;\" alt=\"Decision Tree for Banking Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab20a5-7c12-48a6-be84-dc12d241fb9a",
   "metadata": {},
   "source": [
    "A Decision Tree has three main components:\n",
    "\n",
    "* **Internal nodes:** An internal node in a Decision Tree &mdash; a blue box in the figure above &mdash; refers to a node that represents a decision point where the data is split based on a certain feature's value. Each internal node typically contains (a) a condition based on a feature's threshold (e.g., *Annual_Income < 65,000?*), (b) the number of $samples$ from the training that have reached that node, and (c) the distribution of class labels\n",
    "\n",
    "* **Branches:** a branch represents outcome of a condition of an internal node and corresponds to a feature (e.g., *Education =\"PhD\"*) value or range of values (e.g., *Annual_Income < 65,000?*). Depending on the condition of an internal node, the inner node may have two or more outgoing branches. In the example Decision Tree above, each internal node yields only two branches; however, this is not a fundamental requirement.\n",
    "\n",
    "* **Leaf nodes:** A leaf node in a Decision Tree  &mdash; a green box in the figure above &mdash; represents the end point of a decision path, where no further splitting occurs. It contains the final output or prediction for the data that reaches it. In classification tasks, a leaf node typically holds the most common class label of the data points that have traversed the path to that node. In regression tasks, it stores the average or mean value of the target variable for the data that reaches the leaf. In the example above, each leaf only contains samples from a single class; again, this is not a fundamental requirement.\n",
    "\n",
    "Once we have such a Decision Tree, we can use it to make predictions on new unseen data. For example, consider a new customer (50, PhD, Single, \\$70,000) applying for a credit. Based on our training data of historical information, do we believe this customer will default on their credit? According to the root not of the Decision Tree, we first need to check if the customer is *Single* or *Married*. Since the customer's marital status is *Single*, we follow the left branch of the root node. This brings us to the internal node requiring us to check if the customer's annual income is below \\$65,000. This is not the case, so we follow the right branch. This brings us to a leaf node which means we can make our prediction. As the majority class label &mdash; and in fact the only class label &mdash; in this leaf node is *\"NO\"*, we predict that the customer will not default on their credit. We can confirm this using our `method predict_default_v1()` that implements this simple series of tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fee82ad-0c6a-4938-a7c7-f19ffcec29dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "source": [
    "print(predict_default_v1(50, \"PhD\", \"Single\", 70000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a234a-82f8-447a-831f-f8974583a0fc",
   "metadata": {},
   "source": [
    "### Training Decision Trees\n",
    "\n",
    "The example banking dataset containing information if customers have defaulted on their credit was small and simple enough &mdash; only nine samples and four attributes &mdash; that we could find a working Decision Tree just looking \"long and hard\" enough at data. Of course, in practice, training datasets are much larger containing hundreds of thousands data samples and maybe hundreds of attributes. We therefore need an automated way to learn a Decision Tree from data. Before we delve deeper into different Decision Tree learning algorithms, it is important to highlight two observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39ca59-7fac-4aee-89af-7f44b2eadb96",
   "metadata": {},
   "source": [
    "#### Multiple Possible Decision Trees\n",
    "\n",
    "For any given training dataset, there is generally never a single Decision Tree that maps each sample to its output (class label or regression value) based on the sample's attributes. Particularly if the number of tests &mdash; the internal nodes in the Decision Tree or the `if...else` statements in a naive implementation does not matter &mdash; it is easy to convince yourself that one can always come up with more fine-grained rules. This is particularly true for large datasets, but also for small datasets there are typically multiple Decision Trees that match a given training dataset. For example, for out bank dataset, the following Decision Tree also maps each customer record to the correct label *\"YES\"* or *\"NO\"*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71018b-c057-4377-8de0-04f904c592da",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-02.png\" style=\"margin:auto;\" alt=\"Decision Tree for Banking Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff3dad-42b5-443e-bc6c-c922291b962c",
   "metadata": {},
   "source": [
    "Naturally, we can directly implement this code as another prediction method, this time called `predict_default_v2()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c3de6f-ecb0-4a36-820b-ad423a78e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_default_v2(age, education, marital_status, annual_income):\n",
    "    if education == \"Bachelor\":\n",
    "        return \"YES\"\n",
    "    else:\n",
    "        if age < 50:\n",
    "            return \"NO\"\n",
    "        else:\n",
    "            return \"YES\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90781a58-4f2f-43b4-a177-db2bc1f37e4d",
   "metadata": {},
   "source": [
    "And we can double-check again if this method works correctly by predicting the class label for each training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628b7d12-eabd-4041-bb4f-0e342bbf2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ID 1] NO\n",
      "[ID 2] YES\n",
      "[ID 3] NO\n",
      "[ID 4] NO\n",
      "[ID 5] YES\n",
      "[ID 6] YES\n",
      "[ID 7] YES\n",
      "[ID 8] NO\n",
      "[ID 9] NO\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df_bank.iterrows():\n",
    "    prediction = predict_default_v2(row[\"Age\"], row[\"Education\"], row[\"Marital_Status\"], row[\"Annual_Income\"])\n",
    "    print(f\"[ID {idx+1}] {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627fad4-c349-4ea9-8ce2-60ac8ba21859",
   "metadata": {},
   "source": [
    "Once again, we get the expected prediction as given by the training data. This means that both `predict_default_v1()` and `predict_default_v2()` make the same prediction with respect to the training data. This, in turn, means that both Decision Trees are possible solutions for our training data. While the two trees &mdash; and there are more! &mdash; look different, the important observation is that different trees may yield different predictions for unseen data. For example, consider the same new customer (50, PhD, Single, \\$70,000) applying for a credit. We can now use both auxiliary methods the implement our Decision Tree to make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a954336-00f0-425a-aa31-aebdfd62b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "YES\n"
     ]
    }
   ],
   "source": [
    "print(predict_default_v1(50, \"PhD\", \"Single\", 70000))\n",
    "print(predict_default_v2(50, \"PhD\", \"Single\", 70000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3942965-321c-4dc0-b254-d3e0cf7a1929",
   "metadata": {},
   "source": [
    "For this customer, both Decision Trees make different predictions. Or more generally, two Decision Trees that perform the same on the training dataset, may perform very differently on a test dataset. This implies that some Decision Trees are better than others with respect to some test data. In other words, we are not looking for only *a* Decision Tree but ideally for the *best* Decision Tree for a particular task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1094cb-adcf-4391-bf57-29df2fb6c9b3",
   "metadata": {},
   "source": [
    "#### Complexity of Problem\n",
    "\n",
    "Although we would like to find the best Decision Tree, it turns out that this problem is in fact *NP-hard*. In simple terms, a problem is NP-hard if there is no efficient algorithm &mdash; that is, an algorithm with a polynomial runtime &mdash; that will solve that problem. The problem of finding the best Decision Tree for a dataset is NP-hard because it involves searching through an exponentially large space of possible trees to identify the one that best fits the test data. The key reasons are:\n",
    "\n",
    "* **Exponential search space:** For a given dataset, there are numerous ways to split the data at each internal node based on different features and thresholds. At each level of the tree, the algorithm must evaluate many possible feature thresholds and split points, which leads to an exponential growth in the number of possible Decision Trees as the dataset size and number of features increase.\n",
    "\n",
    "* **Combinatorial nature:** The Decision Tree construction process requires selecting the best feature and threshold at each node, and this decision impacts the structure of the tree. To evaluate all possible combinations of splits across all nodes and features requires checking an enormous number of configurations, which is inherently combinatorial. As a result, no known algorithm can solve this problem in polynomial time, thus making it NP-hard.\n",
    "\n",
    "Summing up, this observation makes exhaustive search impractical for large datasets, which is why any practical implementations of Decision Tree learning algorithms rely on heuristics and greedy algorithms, as they provide near-optimal solutions in much less time. While the resulting Decision Trees are very likely to be good solutions, we have to keep in mind that they are not guaranteed to be the best solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686fe20-0154-48c6-81bc-f4147f17aef9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9965f35-85d1-4207-8bbe-0fecb45768c1",
   "metadata": {},
   "source": [
    "## Building a Decision Tree &mdash; Core Concepts\n",
    "\n",
    "There are several important learning algorithms for Decision Trees &mdash; and we cover the most popular ones later &mdash; but they all share fundamental concepts that derive from the tree structure of the model. In the following, we go through the concepts on an abstract level and outline how different Decision Learning algorithms handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4cecfe-6e48-458b-8c42-918db87e0acf",
   "metadata": {},
   "source": [
    "### Basic Construction Process\n",
    "\n",
    "All Decision Tree learning algorithms follow a top-down approach. When building a Decision Tree, we start with a single root node which contains the entire dataset. For our banking dataset, the root node looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ec85d-f8da-491f-a1e9-0ee93cb639af",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-02a.png\" style=\"margin:auto;\" alt=\"Decision Tree for Banking Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4acc29-f184-4573-a66d-74b3915aa748",
   "metadata": {},
   "source": [
    "Since we do not test any condition that would lead to any branching, our single root node is a leaf node. In principle, we could stop here and use this \"tree\" to make predictions. In case a leaf node contains samples from multiple classes, the majority class is commonly used as the prediction. In other words, we would always predict *\"NO\"* for any training or test sample, independent from its feature values. This is arguably not a very useful model. To make more meaningful predictions that take the features of samples into account, we need to start splitting nodes.\n",
    "\n",
    "Splitting a current leaf node means that the algorithm selects the best feature and threshold (based on some criterion) to split the dataset into smaller subsets. How the best split is determined depends on the specific Decision Tree learning algorithm. For now, let's assume that we have some oracle that tells how to split a leaf node. For example, for our banking dataset, let's assume that the oracle tells us that we should split the root node by check *Education* of a customer is *\"Bachelor\"* or not. This will result in the following split:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f144b95-d855-4e8f-b5b3-461e298d051a",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-02b.png\" style=\"margin:auto;\" alt=\"Decision Tree for Banking Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2ef96-9543-4946-8e4f-fa3f421f65f4",
   "metadata": {},
   "source": [
    "Our root node has now become an internal node as it contains a test on a feature resulting in two branches ending in two new leaf nodes. In our small dataset, there are customers with a *\"Bachelor\"* degree. For these three customers, the split condition is true. Thus, the corresponding three data samples move to the left child node (the \"True\" branch); all remaining six samples (with a *\"Masters\"* or *\"PhD\"* degree) move to the right child node (the \"False\" branch).\n",
    "\n",
    "If we would stop the construction process here, our Decision Tree would already perform fairly well. Assuming we always use the majority class label to make predictions, we would only misclassify only one sample from our training dataset. More specifically, we would misclassify the sample with ID 6 representing the customer (55, Masters, Married, $85,000). This customer has defaulted on their credit but we would predict *\"NO\"* with the Decision Tree above. However, this is not necessarily a bad thing. Maybe this sample is an outlier we should ignore. This idea will later bring us to the concept of pruning and similar strategies to handle the risk of overfitting with Decision Trees. Right now, let's assume we want to find a tree where all leaves contain only samples of a single class label.\n",
    "\n",
    "When looking at our current Decision Tree, we can see that we have already achieved this goal for the left leaf node. All three samples contained in this leaf node have the same class label *\"NO\"*. There is therefore no need for further splitting this node. In contrast, the right leaf node still contains samples of both classes. Hence, we recursively continue the same splitting process by finding the best split for this leaf node. As before, we ask our oracle and it tells us that we should now check if the *Age* if the customer was below $50$ or not. Splitting the leaf node based on this condition will again turn this node into an internal node with to branches pointing two new leaf nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6f3c3-de31-41d3-8347-253fef1f3456",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-02c.png\" style=\"margin:auto;\" alt=\"Decision Tree for Banking Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a680e5b-1f0f-4e51-8c33-5f61f1e48e31",
   "metadata": {},
   "source": [
    "In short, Decision Trees grow by recursively checking all leaf nodes if a split is required and, if so, perform the split. By default, we can stop splitting a leaf node if it only contains samples of the same class label &mdash; or just a single data sample, which is just a special case of the former condition. In practice, the process of splitting may stop before these bases cases if some other predefined stopping criteria is satisfied. For example, we may want to specify that a Decision Tree should not exceed a maximum depth, or that a leaf node has to contain a minimum number of samples. Again, a deeper discussion of such criteria and their purpose is beyond the scope here.\n",
    "\n",
    "If we check both new leaf nodes that were added after the last split, we can see that both satisfy the default stop condition, since both leaf nodes only contain samples of the same class &mdash; only *\"NO\"* in the left child node and only *\"YES\"* in the right child node. Thus, the learning algorithm stops and we have trained our final Decision Tree. Of course, for large datasets, the resulting Decision Tree is likely to be much larger with many more internal and leaf nodes. However, the process of recursively splitting leaf nodes until some stopping criterion is satisfied remains exactly the same.\n",
    "\n",
    "As long as the training data does not contain ambiguous data, we can also construct a Decision Tree where each leaf contains only samples of the same class. Ambiguous data refers to the situation where two or samples have the same feature values but different class labels, for example:\n",
    "\n",
    "| **ID** | **Age** | **Education** | **Marital_Status** | **Annual_Income** | **Credit Default** |\n",
    "|--------|---------|---------------|--------------------|-------------------|--------------------|\n",
    "| ...\t| ... \t| ...       \t| ...            \t| ...           \t| ...            \t|\n",
    "| 6  \t| 55  \t| Masters   \t| Married        \t| 85,000        \t| YES            \t|\n",
    "| ...\t| ... \t| ...       \t| ...            \t| ...           \t| ...            \t|\n",
    "| 102\t| 55  \t| Masters   \t| Married        \t| 85,000        \t| NO             \t|\n",
    "| ...\t| ... \t| ...       \t| ...            \t| ...           \t| ...            \t|\n",
    "\n",
    "In general, this means the available features themselves are insufficient to distinguish between classes, which can be quite common in many applications. For example, in our banking use case, it is not unlikely that two customers may have the same features, but one has defaulted in their credit while the other has not. Such label conflicts will prevent achieving a pure split, leaving leaf nodes with mixed class labels. However, label conflicts due to ambiguous data are a general challenge and not limited to Decision Trees but affect all machine learning models.\n",
    "\n",
    "\n",
    "All popular Decision Tree learning algorithms follow this top-down strategy of starting with a single root node and recursively splitting leaf nodes. The key advantages of strategy are:\n",
    "\n",
    "* **Simplicity and intuitiveness:** The top-down approach mirrors how humans often solve problems: starting broadly and refining decisions step-by-step. It provides a straightforward, systematic method for tree construction.\n",
    "\n",
    "* **Greedy optimization:** The algorithm uses a greedy approach at each node, choosing the split that optimizes a specific criterion (discussed later) for that step. This localized optimization is computationally efficient and reduces the need for complex global optimization techniques.\n",
    "\n",
    "* **Efficiency and scalability:** The top-down approach is computationally efficient, as it processes one level at a time. Since it does not require backtracking, it avoids the overhead of revisiting earlier splits to check their validity or improve them. It can handle large datasets effectively because it progressively reduces the size of subsets at each level. Each split reduces the number of data points considered for the next level, making the process manageable.\n",
    "\n",
    "* **Modularity:** The recursive nature of the top-down approach allows for modular implementation. Each node can be treated as an independent problem, enabling easier debugging and parallelization in some cases.\n",
    "\n",
    "* **Interpretability:** The decisions made at each step are based on well-defined criteria (e.g., splitting on the most informative feature). This step-by-step logic makes the resulting decision tree easy to interpret and explain.\n",
    "\n",
    "* **Adaptability:** The algorithm can adapt to different problems and datasets by tweaking the splitting criteria or stopping conditions (e.g., maximum depth, minimum samples per leaf). Different types of top-down decision tree algorithms can be tailored to suit specific tasks, such as classification or regression.\n",
    "\n",
    "* **Compatibility with pruning:** A tree generated using a top-down approach can be pruned afterward to improve generalization and prevent overfitting. Pruning methods like reduced-error pruning or cost-complexity pruning complement the top-down construction process.\n",
    "\n",
    "While the top-down approach has many advantages, it also has limitations. Most importantly, it may not find the globally optimal tree because it makes greedy decisions at each step without considering the overall structure. It can overfit noisy or imbalanced data if stopping criteria or pruning are not applied effectively. Despite these limitations, the top-down approach remains one of the most practical and widely used methods for decision tree learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052ba7c-ae1d-40ad-a946-51e92a272f4f",
   "metadata": {},
   "source": [
    "### Finding the Best Split\n",
    "\n",
    "When covering the basic top-down strategy for constructing a Decision Tree, we skipped over the task of how to split a leaf node (if necessary). So far, we assumed to have some oracle that tells which condition to use to split the dataset into child nodes. Of course, in practice, such an oracle is not available. Therefore, a Decision Tree learning algorithm has to programmatically find the most suitable condition to split a node. While this is a task where most learning algorithms differ from each other with respect to the exact implementation, the general criterion is the same. Recall that, by default, we stop training a Decision Tree if all leaf nodes contain only samples of the same class. Intuitively, this means that we prefer splits that better separate data samples with respect to their class labels.\n",
    "\n",
    "To illustrate this, let's look again at the root node of our banking dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d20bd-1c25-43fb-802d-4382648725e4",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-02a.png\" style=\"margin:auto;\" alt=\"Decision Tree for Banking Example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5192b-b982-465e-9d1b-0ee628d3de16",
   "metadata": {},
   "source": [
    "For splitting this node, we not only to choose between any of the four inputs features (*Age*, *Education*, *Martial_Status*, *Annual_Income*), but for the selected feature we also have to choose a threshold &mdash; or more since, in general, a split may yield more than two branches. From all those possible, let's just look at two of them. In fact, there are the to roots splits we saw in the previous example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8dfd3-25b5-49a1-8a1c-dbd13de5965a",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-split02.png\" style=\"margin:auto;\" alt=\"Split Alternative 1\">\n",
    "<br /><br />\n",
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-split01.png\" style=\"margin:auto;\" alt=\"Split Alternative 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e986a-aa78-4498-a851-9c021a37f9bf",
   "metadata": {},
   "source": [
    "We saw that both root split yield Decision Trees mapping the samples from the training data to their respective class labels with respect to the samples' feature values. So the question is: Which split do we consider the preferred choice? The rationale of all Decision Trees is that they prefer splits that result in a better separation of the data samples according to their classes. Before the split, we start with a leaf node with a fairly mixed set of class labels (5 *\"NO\"* and 4 *\"YES\"*). Splitting this node based on condition if the *Marital_Status* of a customer is *\"Single\"*, we end up with to child nodes where the class labels are still very much mixed (3 *\"NO\"* and 2 *\"YES\"* in the left child node; 2 *\"NO\"* and 2 *\"YES\"* in the right child node). In simple terms, we didn't really make much progress.\n",
    "\n",
    "In contrast, when splitting based on the condition of a customer has a *\"Bachelor\"* degree, the class labels in the child nodes are much more separated. In fact, the left child node contains only samples of class *\"NO\"*, so no further splitting required here. But also the right child node is already quite separated with only 1 *\"YES\"* beside 5 *\"NO\"*. This means, given these two choices to split the root node, we prefer the second split based on customers *\"Education\"*. For any practical implementation of a Decision Tree Learning algorithm, we need a way to quantify if a split yields a better or worse separation of class labels. And different learning algorithms use different metrics.\n",
    "\n",
    "Still, keep in mind that this approach is only a heuristic since the decision which split is preferred is a \"local\" decision that only depends on the current leaf node in question. This decision does not incorporate any future splits. For example, splitting our root nodes based on customers *\"Martial_Status\"* might seem suboptimal when it comes to separating the class labels, but subsequent split may result in a final Decision Tree that performs better than the one using *\"Education\"* for the split condition in the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9463bc-f995-4cdb-8654-6a11f3c2908f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbf725-625f-40b5-a174-026e59277d6e",
   "metadata": {},
   "source": [
    "## Learning Algorithms for Decision Trees &mdash; Overview\n",
    "\n",
    "We now have a basic understanding of what Decision Trees are and how we can train them for a given dataset. However, the exact process of training a Decision tree depends on the task (classification or regression), the type of data (numerical or categorical), the splitting criteria, and other characteristics. As such, there are a range of popular Decision Tree learning algorithms that differ in their application to accommodate different tasks and types of data, support different splitting criteria, may handle missing values, and so on. To wrap up this introductory notebook, we briefly discuss the main characteristics and provide a brief outline of the most popular Decision tree learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b6264-8127-4290-b5a6-4a34890bdc60",
   "metadata": {},
   "source": [
    "### Characteristics\n",
    "\n",
    "#### Support Tasks\n",
    "\n",
    "Decision Trees supervised machine learning models. However, not all Decision Tree learning algorithms can be used for all tasks. Among the most popular learning algorithms ID3, C4.5/C5.0, and CHAID are suitable for classification tasks. In contrast, M5 can only be applied to regression tasks. CART can be used for both classification and regression tasks.\n",
    "\n",
    "#### Type of Data\n",
    "\n",
    "In principle, Decision Trees can be trained using numerical and/or categorical data; as we have seen in our small banking use case. However, not all learning algorithms actually support both types of data. For example, the native implementations of CART and M5 assume numerical data, while the native implementations of ID3 and CHAID assume categorical data. Of course, we can preprocess the data using\n",
    "\n",
    "* **Encoding (or continuization)** to transform categorical data into numerical data\n",
    "* **Discretization** to transform numerical data into categorical data.\n",
    "\n",
    "Such steps are particularly important if a dataset contains both numerical and categorical features, just like our banking dataset. For example, without first somehow encoding *Education* and *Marital_Status* as numerical features, we would not be able to directly use the Decision Tree implementation of the scikit-learn library, since this implementation is based on the CART learning algorithm. For example, we could use binary encoding the converted the values for *Martial_Status* from *\"Single\"* and *\"Married\"* to $0$ and $1$. If we assume the the values for *\"Education\"* have a natural order, we can use ordinal encoding to convert the values as follows: *\"Bachelor\"* $\\rightarrow 0$, *\"Masters\"* $\\rightarrow 1$, and *\"Bachelor\"* $\\rightarrow 1$. After those transformation steps all four customer features are numerical allowing to train a Decision Tree using the, for example, CART learning algorithm.\n",
    "\n",
    "\n",
    "#### Splitting Criteria\n",
    "\n",
    "As mentioned previously, all Decision Tree Learning algorithms split a node to effectively separate the class labels (in case of classification task). In other words, the overall goal is to find splits that minimize the \"impurity\" of the child nodes, where impurity reflects how many samples of different classes a node contains. For example, a node is considered \"pure\" if all samples in the node are of the same class. In contrast, if all classes appear equally frequent the node is very \"impure\". In a nutshell, different algorithms use different means to measure the impurity of nodes and/or the decrease of impurity caused by a split to find the best split. Without going into detail here, the most common metrics are:\n",
    "\n",
    "* Entropy and entropy-based criteria\n",
    "* Gini impurity\n",
    "* Variance reduction (for regression tasks)\n",
    "\n",
    "While the formula behind the different metrics might vary, their effects can be very similar; after all, they all have the same overall goal. For example, the CART algorithm works with both Entropy and Gini Impurity, and it has been shown that it makes no difference in almost all cases.\n",
    "\n",
    "#### Handling Missing Data\n",
    "\n",
    "So far, we assume that the dataset for training a Decision Trees has no missing values. Unfortunately, missing values are common in real-world data. For example, in our banking dataset, we might not have the Education level for all customers, or unemployed customers cannot report an annual income. In general, handling missing values is a common data preprocessing step since most machine learning algorithms cannot handle missing values and will throw errors. Without going into details common strategies include to simply remove data samples with missing values, or imputation techniques to fill missing values with meaningful estimates.\n",
    "\n",
    "Despite commonly treating the handling of missing values a dedicated preprocessing step, Decision Tree learning algorithms allow for some inherent means to work with missing values without throwing errors. Some common approaches include:\n",
    "\n",
    "* **Fallback rules:** When splitting a node, the calculation of the best split is done always but only based on the samples without missing values for a chosen feature. After the split all samples that have missing values for that feature are moved to any of the child nodes based on predefined rules. A very simple rule may be to always move such samples to the left/first child node. Another common rule is to move such samples to a child node containing the most samples. The intuition is that a sample with a missing value would more likely to be in the \"bigger\" child node if the value would be available.\n",
    "\n",
    "* **Missing values as a new category:** Particularly suitable for categorical features, missing values treated a separate and distinct value (e.g., *\"Single\"*, *\"Married\"*, and *\"Unknown\"* for *Marital_Status*). This way, one can include the records with missing values in the training data, and split them based on the presence or absence of values. This approach can help capture the information and meaning of missing values, and avoid the noise and uncertainty of imputation. However, this approach can also increase the complexity and size of the decision tree, and create spurious and irrelevant splits. Furthermore, this approach can be problematic if the missing values are not random or informative, but rather due to errors or inconsistencies in the data collection or processing.\n",
    "\n",
    "* **Surrogate splits:** Surrogate splits are based on the idea that some variables are highly correlated or similar to each other, and can therefore substitute each other in the splitting process. This approach can help reduce the complexity and size of the decision tree, and avoid the bias and distortion of ignoring missing values. However, this approach also depends on the availability and quality of surrogate variables, and introduces some error and inconsistency in the splitting process. Moreover, this approach can be difficult to interpret and explain, especially if the surrogate variables are not intuitive or logical.\n",
    "\n",
    "None of those basic strategies is intrinsically better than the other. All make certain assumptions, and they all have their pros and cons. In practice, missing values are handled as its own preprocessing step for two main reasons. Firstly, when finding a best classification or regression model, other models beyond Decision Trees are also considered, and most of them cannot deal with missing values. And secondly, handling missing values allows for more control and more sophisticated strategies (e.g., data imputation techniques).\n",
    "\n",
    "#### Counter-Measures to Overfitting\n",
    "\n",
    "Decision trees are particularly prone to overfitting because they are highly flexible and can grow very complex, capturing even the smallest details of the training data. A decision tree splits the data recursively at each node based on the feature that best separates the data at that point. Without constraints, the tree continues to grow until it perfectly classifies the training data, often resulting in deep trees with many branches. These deep trees can learn patterns that are specific to the training data, including noise and outliers, rather than capturing generalizable trends.\n",
    "\n",
    "Another reason for their susceptibility to overfitting is their greedy nature in selecting splits. At each step, a decision tree chooses the best possible split based on a local criterion (e.g., Gini impurity or information gain), without considering the broader picture or the potential for overfitting. This can lead to overly complex trees that perform well on the training set but fail to generalize to new, unseen data. Additionally, decision trees can become sensitive to small changes in the data, as a minor variation can lead to entirely different splits, further reducing their robustness.\n",
    "\n",
    "Since overfitting in Decisions is typically a result of deep and complex trees, counter-measures to lower the risk of overfitting therefore focus on reducing the complexity of a tree. This is can be done in two main ways:\n",
    "\n",
    "* **Early stopping (pre-pruning):** When growing a Decision Tree by recursively splitting leaf nodes, early stopping or pre-pruning means that the learning algorithm will not split a node if a certain condition is satisfied &mdash; of course, beyond the default condition that the leaf node is pure, i.e., it contains only samples of the same class. Common such conditions are:\n",
    "\t* *Maximum depth:* a leaf node is no longer split if the path to the root node exceeds a predefined threshold\n",
    "\t* *Minimum sample size*: a leaf node is no longer split if the number of sample the node contains is below a predefined threshold\n",
    "\t* *Minimum improvement*: a leaf node is no longer split if the split does no result in a sufficiently better separation of the class labels in the child nodes\n",
    "* **Post-pruning:** Post-pruning a decision tree means growing the tree fully during training and then removing unnecessary branches afterward to simplify the model. The goal is to strike a balance between capturing the structure of the data and avoiding overfitting. After the tree is fully grown, post-pruning evaluates each branch and removes those that do not significantly improve the tree's performance on a validation dataset. Pruning decisions are typically based on metrics like accuracy, error rate, or a cost-complexity tradeoff. For example, a branch might be pruned if its removal reduces the complexity of the tree without significantly increasing the error rate.\n",
    "\n",
    "Both pre-pruning and post-pruning rely on validation data and strategies such as cross validation to find the best values for the maximum depth, the minimum samples, etc., as well as to decide whether a branch should be removed from the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74362f7a-d772-4467-a91d-d1dc5f31a199",
   "metadata": {},
   "source": [
    "### Common Algorithms\n",
    "\n",
    "Let's have a brief look at some of the most popular Decision Tree learning algorithms. A deep dive into their inner workings are the topics on their own and covered in separate notebooks. We also focus on the original/native implementations of the algorithms. Their implementations available in libraries or toolkits may be slightly modified towards further improvement. For while a native implementation of an algorithm cannot handle missing values, an improved version of that algorithm might be able to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83373c9-dc41-4300-bbca-c50f5782a717",
   "metadata": {},
   "source": [
    "#### ID3 (Iterative Dichotomiser 3)\n",
    "\n",
    "The ID3 algorithm is primarily designed for categorical data; features with (continuous) numerical features need to be discretized (e.g., converting the dollar values of *Annual_Income* to *\"low\"*, *\"mid\"*, and *\"high\"*). ID3 splits nodes by creating a new branch and child node for each possible value for the selected features. For example, ID3 would split a node using *Education* into three child nodes for *\"Bachelor\"*, *\"Masters\"*, and *\"PhD\"*; see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397047d3-ba29-4b9d-8c3d-d87ac148c42f",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/illustrations/decision-trees/dt-banking-example-id3.png\" style=\"margin:auto;\" alt=\"Decision Tree ID3 Example Split\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238277dd-dd3e-43a4-a80a-e2f52ff53758",
   "metadata": {},
   "source": [
    "This means that, along the path from a leaf node to the root node, each feature will only be used once for a split. For example, given the example split above, when we want to further split the leaf node for the *\"Masters\"* branch, there is no need to consider *\"Education\"* anymore &mdash; after all we know that all samples in this leaf node have the same value for *\"Education\"*. Other Decision Tree Learning algorithms may use the same feature for splits along the path from a leaf to the root node.\n",
    "\n",
    "To find the best feature to be used for splitting a leaf node, ID3 uses information gain as a criterion. It measures how well a particular attribute separates the data into distinct classes &mdash; note that we do not need to consider any additional threshold here. Information gain is based on a concept from information theory called entropy, which measures the uncertainty or randomness in a dataset. By selecting the attribute that reduces entropy the most (i.e., provides the highest information gain), the ID3 algorithm ensures that each split improves the tree's ability to make accurate predictions.\n",
    "\n",
    "One major advantage of the ID3 learning algorithm is its simplicity and ease of implementation, which makes it an excellent introduction to decision tree algorithms. Additionally, it is computationally efficient for small to medium-sized datasets with categorical attributes, making it practical for many real-world applications. However, ID3 also has notable limitations. A key drawback is its tendency to overfit the data, especially when there are attributes with many unique values or when the tree becomes too complex. It does not inherently include mechanisms like pruning to simplify the tree and prevent overfitting. Furthermore, ID3 struggles with continuous numerical data, as it only handles categorical attributes directly. Converting numerical data into categories can result in a loss of information. It also does not account for missing values in the dataset, which can lead to inaccuracies in model training and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2591b-69a0-4528-a3b9-13835edb8fea",
   "metadata": {},
   "source": [
    "#### C4.5/C5.0\n",
    "\n",
    "The C4.5 (and its successor, C5.0) decision tree algorithms are advanced methods for building decision trees and are improvements over the earlier ID3 algorithm. They are primarily used for classification tasks. Unlike ID3, which uses information gain, C4.5 introduces the gain ratio to address a common issue where attributes with many unique values tend to dominate splits. The gain ratio normalizes the information gain by the \"split information,\" ensuring that the algorithm selects attributes that are truly meaningful and not just overly specific.\n",
    "\n",
    "C4.5 also stands out because it can handle both categorical and numerical attributes. For numerical data, it finds the best threshold to split the data, which makes it more versatile than ID3. Additionally, C4.5 can handle datasets with missing values by estimating the most probable values based on the existing data. This feature makes it more robust when working with real-world data, where missing values are common. Another significant improvement in C4.5 is its ability to prune the tree after it is built to address overfitting issues. The later version, C5.0, further improves on C4.5 by being faster, more memory-efficient, and better at handling large datasets.\n",
    "\n",
    "However, there are some limitations to these algorithms. C4.5 can be computationally intensive for very large datasets, especially when dealing with numerous features or attributes. While C5.0 addresses some of these efficiency issues, the complexity of tuning hyperparameters (e.g., pruning thresholds or boosting settings) can still be a challenge. Additionally, decision trees built with these algorithms are sensitive to noisy data, as even small variations can lead to different splits, which may affect the models stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8245e7-2958-4ee1-9789-4c08329eec7f",
   "metadata": {},
   "source": [
    "#### CART (Classification & Regression Trees)\n",
    "\n",
    "The CART learning algorithm is used for both classification tasks (where the goal is to predict categories) and regression tasks (where the goal is to predict numerical values). Unlike ID3, it builds binary trees, meaning every split in the tree divides the data into exactly two parts. Most implementations of CART, including the one provided by the popular scitkit-learn library also assume that all features are numerical, potentially requiring the encoding/continuization of categorical features.\n",
    "\n",
    "For classification problems, CART typically uses a metric called Gini impurity to choose the best attribute to split the data. Gini impurity measures how \"mixed\" the data is in terms of different classes; the goal is to reduce this impurity as much as possible with each split. For regression problems, CART uses the mean squared error (MSE) to determine splits, focusing on minimizing the difference between predicted and actual values.\n",
    "\n",
    "Beyond its versatility to be used for classification and regression tasks, the CART algorithms also built-in mechanisms to support pruning to tackle overfitting, as well as handling missing values. However, CART also has limitations. Binary splits can sometimes lead to deeper trees compared to algorithms that allow multi-way splits, potentially increasing computational cost and complexity. While CART is less prone to overfitting than some other algorithms due to pruning, it still requires careful tuning of parameters like tree depth and minimum samples per split. Additionally, CART may struggle with very imbalanced datasets, where one class dominates, as it does not inherently include methods to handle such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82925bc6-17c5-4d53-ae41-617720e98c73",
   "metadata": {},
   "source": [
    "#### CHAID (Chi-Square Automatic Interaction Detection)\n",
    "\n",
    "The CHAID Decision Tree learning algorithm is a statistical technique used for classification and prediction. It is unique because it relies heavily on statistical tests to determine how to split data at each step, rather than using measures like information gain or Gini impurity, as other decision tree algorithms do. Specifically, CHAID uses the chi-squared test (for categorical variables), the F-test (for continuous dependent variables), or the Bonferroni adjustment to find the most statistically significant splits.\n",
    "\n",
    "One of the most distinctive features of CHAID is its ability to perform multi-way splits at each node, rather than just binary splits. This means that instead of dividing data into only two groups, CHAID can create multiple branches based on the number of significant categories in the data. For example, if an attribute has four categories and all are statistically relevant, CHAID might create four branches at a single node. This results in shallower trees that are easier to interpret compared to algorithms that create deep binary trees.\n",
    "\n",
    "Another unique aspect of CHAID is that it handles categorical and ordinal data particularly well, making it popular in fields like marketing and social sciences. It evaluates all potential splits and merges categories with similar statistical properties, simplifying the tree while retaining meaningful distinctions in the data. Additionally, CHAID can handle missing values by treating them as a separate category or by using statistical estimates.\n",
    "\n",
    "As a downside, CHAID assumes that the data relationships are adequately represented by the statistical tests it performs, which might not always capture complex patterns. Additionally, because CHAID heavily relies on statistical significance, it can be sensitive to small sample sizes or noisy data. These limitations mean that CHAID is most effective when used with clean, well-prepared datasets and when the goal is interpretability rather than high predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4400db-bf12-43b4-bf3b-75792ba6a33d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354bfd4-4d6f-472c-8777-0dbfbc14cf9e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "A decision tree is a type of machine learning model that mimics human decision-making by representing decisions and their possible consequences in a tree-like structure. Each internal node in the tree represents a question or decision point based on an attribute, each branch represents a possible outcome of that decision, and each leaf node represents a final prediction or outcome. The basic idea is to split the dataset into smaller, more homogenous groups at each step, making it easier to classify data or make predictions.\n",
    "\n",
    "One of the key reasons decision trees are important is their interpretability. Unlike many complex machine learning models, decision trees are easy to visualize and understand. They provide clear rules for how predictions are made, which is valuable in fields like healthcare, finance, or any domain where understanding the reasoning behind predictions is critical. This interpretability makes them especially useful for communicating results to non-technical stakeholders.\n",
    "\n",
    "Decision trees are also versatile. They can handle both classification tasks, such as determining if an email is spam or not, and regression tasks, like predicting house prices. Additionally, they work well with both categorical and numerical data. Many advanced machine learning techniques, like random forests and gradient boosting machines, are built on the foundation of decision trees, making them a crucial concept to understand for anyone studying machine learning.\n",
    "\n",
    "However, decision trees are not without limitations. They are prone to overfitting, where the model becomes too complex and captures noise in the training data instead of general patterns. Techniques like pruning and ensemble methods (combining multiple trees) help address this issue. Despite these challenges, decision trees remain a foundational and widely used tool in the machine learning toolbox. In short, decision trees are essential because of their simplicity, interpretability, and versatility. They serve as a starting point for understanding more complex models and play a crucial role in the development of many advanced machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7dc74a-7f6d-4de1-b3a6-8b4a3dcfdbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
