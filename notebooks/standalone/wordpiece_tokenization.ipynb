{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b06ea1",
   "metadata": {},
   "source": [
    "### Auto-Generated Code Cells\n",
    "\n",
    "The following code cells contain code from external .py files that has been automatically added to remove these dependencies. This allows you to run this version of the SELENE notebook on Cloud platforms such as Google Colab. Run all following code cells before starting with the actual notebook content. \n",
    "\n",
    "Most Cloud platforms such as Google Colab have a wide range of Python packages preinstalled. However, if any import statements in the following code cells throws an error because of a missing package, you can create and run a new code cell to install the missing package using the `pip` command as shown here:\n",
    "```\n",
    "!pip install <package-name>\n",
    "```\n",
    "\n",
    "You can use the code cell below for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0971d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install <package-name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inline content for module src.utils.data.files  ==\n",
    "\n",
    "# === Inlined imports ===\n",
    "from tqdm import tqdm\n",
    "import os, re, yaml\n",
    "import bz2\n",
    "import zipfile, tarfile\n",
    "import requests\n",
    "\n",
    "# === Embedded configs ===\n",
    "CONFIG = yaml.safe_load(\"\"\"urls:\n",
    "  downloads:\n",
    "    notebooks: \"https://github.com/chrisvdweth/selene/tree/master/notebooks\"\n",
    "    datasets: \"https://chrisvdw.net/projects/selene/downloads/datasets/\"\n",
    "    models: \"https://chrisvdw.net/projects/selene/downloads/models/\"\n",
    "    \"\"\")\n",
    "\n",
    "# === Inlined definitions ===\n",
    "def download_file(url, download_path, overwrite=False, ignore_html=False):\n",
    "    file_name = url.split('/')[-1]\n",
    "    create_folder(download_path)\n",
    "    file_path = download_path + file_name\n",
    "    if os.path.isfile(file_path) == True and overwrite is not True:\n",
    "        print(f\"File '{file_path}' already exists (use 'overwrite=True' to overwrite it).\")\n",
    "        return (file_path, download_path)\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            if ignore_html is True and is_html_file(data) is True:\n",
    "                print('Error downloading file (expected data file, got HTML file)')\n",
    "                return (None, None)\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print('Error downloading file (source does not exist)')\n",
    "        return (None, None)\n",
    "    return (file_path, download_path)\n",
    "\n",
    "def is_html_file(content):\n",
    "    content = content.decode('utf-8', 'ignore').strip().lower()\n",
    "    if content.startswith('<!doctype html') is True:\n",
    "        return True\n",
    "    elif content.startswith('<html') is True:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def download_dataset(dataset_path, base_url=None, download_path=None, overwrite=False, ignore_html=True):\n",
    "    if base_url is None:\n",
    "        base_url = CONFIG['urls']['downloads']['datasets']\n",
    "    if download_path is None:\n",
    "        download_path = 'data/datasets/' + '/'.join(dataset_path.split('/')[0:-1]) + '/'\n",
    "    url = base_url + dataset_path\n",
    "    return download_file(url, download_path=download_path, overwrite=overwrite, ignore_html=ignore_html)\n",
    "\n",
    "def create_folder(folder_name, exist_ok=True):\n",
    "    try:\n",
    "        os.makedirs(folder_name, exist_ok=exist_ok)\n",
    "        return folder_name\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a524c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inline content for module src.text.preprocessing.tokenizing  ==\n",
    "\n",
    "# === Inlined imports ===\n",
    "from tqdm import tqdm\n",
    "import re, collections, regex\n",
    "\n",
    "# === Embedded configs ===\n",
    "\n",
    "\n",
    "# === Inlined definitions ===\n",
    "class MyWordPieceTokenizer:\n",
    "    PRE_TOKENIZE__SPLIT = 0\n",
    "    PRE_TOKENIZE__GPT2 = 1\n",
    "\n",
    "    def __init__(self, ctoken='##', pretokenize=PRE_TOKENIZE__SPLIT):\n",
    "        self._pretokenize = pretokenize\n",
    "        self._ctoken = ctoken\n",
    "        self._vocabulary = {}\n",
    "        self._corpus_state = {}\n",
    "        self._merges = []\n",
    "\n",
    "    def _init(self, docs: list):\n",
    "        self._vocabulary = set()\n",
    "        self._corpus_state = collections.defaultdict(int)\n",
    "        self._merges = []\n",
    "        for doc in docs:\n",
    "            for word in self._pretokenize_text(doc):\n",
    "                for idx, char in enumerate(word):\n",
    "                    if idx == 0:\n",
    "                        self._vocabulary.add(char)\n",
    "                    else:\n",
    "                        self._vocabulary.add(f'{self._ctoken}{char}')\n",
    "                self._corpus_state[self._generate_sequence(word)] += 1\n",
    "\n",
    "    def _pretokenize_text(self, text):\n",
    "        if self._pretokenize == MyWordPieceTokenizer.PRE_TOKENIZE__SPLIT:\n",
    "            return text.split()\n",
    "        elif self._pretokenize == MyWordPieceTokenizer.PRE_TOKENIZE__GPT2:\n",
    "            gpt2pattern = regex.compile(\"'s|'t|'re|'ve|'m|'ll|'d|\\\\p{L}+|\\\\p{N}+|[^\\\\s\\\\p{L}\\\\p{N}]+\")\n",
    "            return regex.findall(gpt2pattern, text)\n",
    "        else:\n",
    "            raise Exception('Unknown pretokenization method.')\n",
    "\n",
    "    def _generate_sequence(self, word):\n",
    "        return ' '.join([c if i == 0 else f'{self._ctoken}{c}' for i, c in enumerate(word)])\n",
    "\n",
    "    def _find_best_token_pair(self):\n",
    "        token_counts = collections.defaultdict(int)\n",
    "        token_pair_counts = collections.defaultdict(int)\n",
    "        for word, freq in self._corpus_state.items():\n",
    "            sequence = word.split()\n",
    "            if len(sequence) == 1:\n",
    "                token_counts[sequence[0]] += freq\n",
    "                continue\n",
    "            for i in range(len(sequence) - 1):\n",
    "                pair = (sequence[i], sequence[i + 1])\n",
    "                token_counts[f'{sequence[i]}'] += freq\n",
    "                token_pair_counts[pair] += freq\n",
    "            token_counts[sequence[-1]] += freq\n",
    "        token_pair_scores = {' '.join(pair): count / (token_counts[pair[0]] * token_counts[pair[1]]) for pair, count in token_pair_counts.items()}\n",
    "        return max(token_pair_scores.keys(), key=lambda key: token_pair_scores[key])\n",
    "\n",
    "    def _create_new_token(self, token_pair):\n",
    "        t1, t2 = token_pair.split()\n",
    "        return ''.join([t1, re.sub(self._ctoken, '', t2)])\n",
    "\n",
    "    def _perform_merge(self, token_pair):\n",
    "        new_token = self._create_new_token(token_pair)\n",
    "        merge = (token_pair, new_token)\n",
    "        self._vocabulary.add(new_token)\n",
    "        pattern = re.compile('(?<!\\\\S)' + re.escape(token_pair) + '(?!\\\\S)')\n",
    "        matches = {}\n",
    "        for sequence, count in self._corpus_state.items():\n",
    "            for match in pattern.finditer(sequence):\n",
    "                matches[sequence] = pattern.sub(new_token, sequence)\n",
    "        for old, new in matches.items():\n",
    "            self._corpus_state[new] = self._corpus_state.pop(old)\n",
    "        return merge\n",
    "\n",
    "    def fit(self, docs, max_vocab_size=100, verbose=False):\n",
    "        if verbose == True:\n",
    "            print('Initilize corpus and vocabulary...')\n",
    "        self._init(docs)\n",
    "        num_iter = max(0, max_vocab_size - len(self._vocabulary))\n",
    "        if verbose == True:\n",
    "            print(f'Perform {num_iter} iterations...')\n",
    "        for _ in tqdm(range(num_iter)):\n",
    "            try:\n",
    "                top_token_pair = self._find_best_token_pair()\n",
    "            except:\n",
    "                break\n",
    "            merge = self._perform_merge(top_token_pair)\n",
    "            self._merges.append(merge)\n",
    "        return self\n",
    "\n",
    "    def tokenize(self, doc: str):\n",
    "        pretokens = self._pretokenize_text(doc)\n",
    "        tokens = []\n",
    "        for pt in pretokens:\n",
    "            tokens.extend(self._tokenize_word(pt))\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize_word(self, word):\n",
    "        sequence = self._generate_sequence(word)\n",
    "        for p, m in self._merges:\n",
    "            if p not in sequence:\n",
    "                continue\n",
    "            p = re.compile('(?<!\\\\S)' + re.escape(p) + '(?!\\\\S)')\n",
    "            sequence = p.sub(m, sequence)\n",
    "        return sequence.split(' ')\n",
    "\n",
    "    def detokenize(self, tokens: list):\n",
    "        doc = ' '.join(tokens)\n",
    "        return re.sub(f' {self._ctoken}', '', doc).strip()\n",
    "\n",
    "def fit(self, docs, max_vocab_size=100, verbose=False):\n",
    "    if verbose == True:\n",
    "        print('Initilize corpus and vocabulary...')\n",
    "    self._init(docs)\n",
    "    num_iter = max(0, max_vocab_size - len(self._vocabulary))\n",
    "    if verbose == True:\n",
    "        print(f'Perform {num_iter} iterations...')\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        try:\n",
    "            top_token_pair = self._find_best_token_pair()\n",
    "        except:\n",
    "            break\n",
    "        merge = self._perform_merge(top_token_pair)\n",
    "        self._merges.append(merge)\n",
    "    return self\n",
    "\n",
    "def detokenize(self, tokens: list):\n",
    "    doc = ' '.join(tokens)\n",
    "    return re.sub(f' {self._ctoken}', '', doc).strip()\n",
    "\n",
    "def tokenize(self, doc: str):\n",
    "    pretokens = self._pretokenize_text(doc)\n",
    "    tokens = []\n",
    "    for pt in pretokens:\n",
    "        tokens.extend(self._tokenize_word(pt))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a002f-e472-40ed-83c3-1c55e25deaa9",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/chrisvdweth/selene/refs/heads/master/notebooks/images/logo/selene-logo-640.png\" style=\"max-height:75px;\" alt=\"SELENE Logo\" />\n",
    "\n",
    "**Disclaimer:** This Jupyter Notebook contains content generated with the assistance of AI. While every effort has been made to review and validate the outputs, users should independently verify critical information before relying on it. The SELENE notebook repository is constantly evolving. We recommend downloading or pulling the latest version of this notebook from Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddee36-f14f-4a0e-967b-60f257dd5145",
   "metadata": {},
   "source": [
    "# Subword Tokenization (WordPiece)\n",
    "\n",
    "The WordPiece algorithm is a widely used method for subword tokenization, particularly in natural language processing (NLP) tasks. Originally introduced for speech recognition, WordPiece gained prominence with its use in pre-trained language models such as BERT. The algorithm addresses key challenges in text representation, such as handling out-of-vocabulary (OOV) words and efficiently representing rare words, by breaking text into subword units instead of relying solely on full words or individual characters.\n",
    "\n",
    "At its core, WordPiece seeks to build a vocabulary of subword units by balancing frequency and efficiency. The algorithm begins with an initial vocabulary consisting of individual characters and iteratively merges pairs of tokens that maximize the likelihood of the training data. This likelihood is computed based on the frequency of token pairs and their impact on the overall representation. The result is a compact vocabulary that includes common words as single units and decomposes rare or complex words into smaller, meaningful subword components. For example, a word like *\"unbelievable\"* might be tokenized into [*\"un\"*, *\"##believable\"*], where the *\"##\"* prefix indicates that the subword is part of a larger word.\n",
    "\n",
    "One of the key strengths of WordPiece is its ability to balance generalization and specificity. By using subwords, the algorithm can effectively handle new words that were not seen during training by combining known subword units. This property reduces the OOV problem and enables models to better understand rare words or morphologically rich languages. Additionally, subword tokenization allows for smaller vocabulary sizes compared to full-word tokenization, which reduces memory requirements and computational costs.\n",
    "\n",
    "WordPiece has become a cornerstone in modern NLP pipelines due to its efficiency and adaptability. It enables pre-trained language models to achieve state-of-the-art performance across various tasks, including text classification, machine translation, and question answering. Its design, which merges data-driven insights with linguistic intuition, has inspired other tokenization algorithms like Byte Pair Encoding (BPE) and SentencePiece. As a result, WordPiece continues to be an essential component in advancing NLP technologies.\n",
    "\n",
    "In this notebook, we will take a closer look at the WordPiece algorithm and implement a basic WordPiece tokenizer from scratch in a step-by-step and illustrative manner.\n",
    "\n",
    "**Note:** The two subword tokenization algorithms WordPiece and Byte-Pair Encoding are very similar. Since BPE is arguably slightly simpler, and we occasionally refer to BPE in this notebook &mdash; to highlight the fundamental difference between WordPiece and BPE &mdash; we recommend first reading up on BPE. However, this is not required to understand how WordPiece works, only to appreciate how it compares to BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e7681-800b-44a7-9266-99b83ceb3062",
   "metadata": {},
   "source": [
    "### Setting up the Notebook\n",
    "\n",
    "#### Make Required Imports\n",
    "\n",
    "This notebook requires the import of different Python packages but also additional Python modules that are part of the repository. If a package is missing, use your preferred package manager (e.g., [conda](https://anaconda.org/anaconda/conda) or [pip](https://pypi.org/project/pip/)) to install it. If the code cell below runs with any errors, all required packages and modules have successfully been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a37d0c6-d2f3-46c2-ac8f-c5cc26ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, regex, collections, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d2bb0-283f-48a5-ad7d-12106cd5a5ac",
   "metadata": {},
   "source": [
    "#### Download Required Data\n",
    "\n",
    "Some code examples in this notebook use data that first need to be downloaded by running the code cell below. If this code cell throws any error, please check the configuration file `config.yaml` if the URL for downloading datasets is up to date and matches the one on Github. If not, simply download or pull the latest version from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5574145-fb39-4f66-a011-84f18fc5875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/datasets/text/corpora/books/treasure-island.txt' already exists (use 'overwrite=True' to overwrite it).\n"
     ]
    }
   ],
   "source": [
    "treasure_island_book, _ = download_dataset(\"text/corpora/books/treasure-island.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65cf2d-5f43-4196-8f19-32a5ca95aba3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9facf37-b0c0-42ca-951e-c83522d3352d",
   "metadata": {},
   "source": [
    "## WordPiece from Scratch\n",
    "\n",
    "The fundamental idea behind WordPiece is quite straightforward to understand and easy to implement, and will go through the algorithm step by step in the following. Practical implementations of WordPiece will be more sophisticated as they consider additional refinements or aim for more efficient implementations. Here, the focus is on the understanding of WordPiece for tokenization. This includes that we use a very artificial example document to better illustrate the inner workings of the algorithm. This example document is defined in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a89666-8c04-4ba3-839a-8c5bfbb38439",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'low low low low low lower lower newest newest newest newest newest newest widest widest widest longer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598a0cc-6660-4d93-9b50-7d10a6326477",
   "metadata": {},
   "source": [
    "The algorithm also needs some special character to mark continuation of a word; its purpose will be clear once we go through the algorithm. Again, to keep it simple, we simply use the underscore character `_` for this. Note that this means that our input document is not allowed to contain underscore characters. This works perfectly fine for our example document here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70986a1-5b32-4196-844d-137cc69c07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CTOKEN = '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef57018-689d-4837-9024-25a57a670a2c",
   "metadata": {},
   "source": [
    "### Core Steps\n",
    "\n",
    "We first go through the core steps of WordPiece, before combining them to the final learning algorithm.\n",
    "\n",
    "#### Pretokenize Text\n",
    "\n",
    "WordPiece assumes that the corpus has been pretokenized into a initial list of tokens. The most basic approach is to pretokenize a text based on whitespace characters &mdash; in practice, slightly more sophisticated methods are used (discussed later). The code cell below contains the method `pretokenize()` the uses the built-in Python method `split()` to convert a text string into a list of tokens by splitting the string with respect to whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84c87e4-75eb-4c4e-ae36-f66a6455df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(doc):\n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd8a22-bd54-4756-9da5-da44b69f3941",
   "metadata": {},
   "source": [
    "To show an example, we can apply this method to our example document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9479c93f-08f5-4818-9c40-f11b9bdf0a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'low', 'low', 'low', 'low', 'lower', 'lower', 'newest', 'newest', 'newest', 'newest', 'newest', 'newest', 'widest', 'widest', 'widest', 'longer']\n"
     ]
    }
   ],
   "source": [
    "print(pretokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5803613-8dad-4b77-aad3-d798a301a021",
   "metadata": {},
   "source": [
    "#### Initialize Corpus State and Vocabulary\n",
    "\n",
    "The corpus state represents a data structure, here a dictionary, that maps each unique token after pretokenization to its number of occurrences on the training corpus. Since WordPiece is a bottom-up approach, we also need to split each token (typically a word) into its characters, and the `CTOKEN` is added to each character except the first one. Let's first create a utility method that performs this step for an initial word/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f886b46e-2793-4562-8e98-51e277b87428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f _a _s _t _e _s _t'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sequence(word, ctoken=CTOKEN):\n",
    "    return ' '.join([c if i == 0 else f\"{ctoken}{c}\" for i, c in enumerate(word)])\n",
    "    \n",
    "generate_sequence('fastest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156507f-339c-4dbd-b89f-dc5185c7bf18",
   "metadata": {},
   "source": [
    "Appreciate the use of the special token to mark the continuation of a word (here the underscore character). Any token starting with that special character represents *not* the beginning of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcd6d6-8e2a-47a9-a1bc-91ce1da5f92c",
   "metadata": {},
   "source": [
    "Now we can go through our all documents to initialize the **corpus state**, i.e., the set of unique words/tokens across the whole corpus converted to their sequences. This for each document we perform the following steps:\n",
    "\n",
    "* Perform simple pretokenization using the `pretokenize()` method.\n",
    "* Split each token into its characters using the utility method `generate_sequence()`.\n",
    "* Add the generate sequence to the corpus state; to avoid repeated entries of the same sequence, we also keep track of the number of times a token appears in the corpus &mdash; we therefore implement the corpus state as dictionary with the sequences as the keys and the number of occurrences as the values.\n",
    "\n",
    "During this process, we also initialize the vocabulary as the set of all unique characters. Later, the learning algorithm will add strings longer than a single character to the vocabulary. Note, however, that the vocabulary itself is not crucial for the WordPiece algorithm itself.\n",
    "\n",
    "The method `initialize_corpus_state()` in the code cell below implements both steps and returns the initial corpus state and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a007ef1-1a12-4e55-b451-2a08d4323c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_corpus_state(docs: list):\n",
    "    # Initial vocabulary as an empty set\n",
    "    vocabulary = set()\n",
    "    # Initialize dictionary representing the corpus state\n",
    "    corpus_state = collections.defaultdict(int)\n",
    "    # Loop over all documents\n",
    "    for doc in docs:\n",
    "        # For each word in the document, generate the sequence and update add it to the corpus state\n",
    "        for word in pretokenize(doc):\n",
    "            # Update vocabulary\n",
    "            for idx, char in enumerate(word):\n",
    "                if idx == 0:\n",
    "                    vocabulary.add(char)\n",
    "                else:\n",
    "                    vocabulary.add(f\"{CTOKEN}{char}\")\n",
    "            # Update sequence count\n",
    "            corpus_state[generate_sequence(word)] += 1\n",
    "    return dict(corpus_state), vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4605377-9510-43d8-b992-75b94b701fa8",
   "metadata": {},
   "source": [
    "We can now run the method over our toy corpus which is just a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c88cb4-4815-4701-9a01-8dad5ad34ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_state, vocabulary = initialize_corpus_state([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c4c22-b4d4-41e1-96a4-a2ada69088fc",
   "metadata": {},
   "source": [
    "Let's first have a look at the corpus state; we use the `dumps()` method of the json library for a more user-friendly output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968c96af-b807-46ec-be44-86efb8690930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"l _o _w\": 5,\n",
      "  \"l _o _w _e _r\": 2,\n",
      "  \"n _e _w _e _s _t\": 6,\n",
      "  \"w _i _d _e _s _t\": 3,\n",
      "  \"l _o _n _g _e _r\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(corpus_state, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ff5c0-0bf2-429e-b47f-e0478a319ec9",
   "metadata": {},
   "source": [
    "When looking at the result, you should recognize all five unique words/tokens that appear in our toy document, only converted to their initial sequences. The number reflects how often that word/token appeared in the document. For example, the key-value pair `\"w _i _d _e _s _t\": 3` indicates that the word *\"widest\"* appeared three times in the document.\n",
    "\n",
    "We can also look at the initial vocabulary which is simply the set of all unique characters in the document, together with the special `CTOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fce62a-1714-4ed1-b6ae-1f1311a3f177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_o', '_e', '_r', '_w', '_i', '_g', 'l', 'w', '_t', '_s', '_d', 'n', '_n'}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44305be6-f3d4-42ef-87bd-01daa9d1af79",
   "metadata": {},
   "source": [
    "If we would do nothing else &mdash; that is, not actually perform any training &mdash; the WordPiece tokenizer would behave like a character tokenizer. We will actually try this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0b0fb-3961-4650-9ba3-946045ff5024",
   "metadata": {},
   "source": [
    "#### Token Merging Step\n",
    " \n",
    "The key idea of WordPiece is to iteratively merge the most frequent token pairs in the corpus state to build a vocabulary that efficiently represents the structure of the input text. The goal is to identify and represent commonly occurring patterns &mdash; such as subwords, roots, prefixes, or suffixes &mdash; with single tokens. This reduces the number of tokens needed to encode the text while maintaining flexibility for representing rare or unseen words through combinations of smaller subword units.\n",
    "\n",
    "By iteratively merging the most frequent adjacent token pairs, WordPiece strikes a balance between character-level and word-level tokenization. Character-level tokenization ensures coverage for any text but can result in extremely long sequences that are computationally expensive to process. On the other hand, word-level tokenization creates inefficiencies when encountering rare words, as each unique word would need a separate token. WordPiece bridges this gap by creating a subword-level tokenization scheme: frequent word components are merged into single tokens, while rare or out-of-vocabulary words can still be represented as sequences of smaller subword units.\n",
    "\n",
    "##### Calculate Number of Token Pairs\n",
    "\n",
    "WordPiece and BPE follow a very similar bottom-up approach by first splitting words into character tokens and then iteratively finding the next best pair of tokens to be merged tokens into a new larger token. While there are minor differences in the general setup, both algorithms fundamentally only differ in the criterion they use to find the next token pair to merge. BPE simply picks the token pair with the most occurrences in the corpus state. For example, assuming the initial corpus state from above, BPE calculates the number of occurrences of *\"l _o\"*, *\"_o _w\"*, *\"_w _e\"*, *\"_e _r\"*, *\"n _e\"*, and so on, and then merges the token pair with the highest number of occurrences (randomly breaking ties).\n",
    "\n",
    "In contrast to BPE, WordPiece does not choose the most frequent token pair, but the one that maximizes the likelihood of the training data once added to the vocabulary. The likelihood of a token pair $(t_1, t_2)$ is calculated as follows:\n",
    "\n",
    "$$\\large\n",
    "\\frac{P(t_1, t_2)}{P(t_1) P(t_2)}\n",
    "$$\n",
    "\n",
    "where $P(t_1, t_2)$ is the probability of seeing the token pair $(t_1, t_2)$ given the corpus, $P(t_1)$ is the probability of seeing token $t_1$ in the corpus, $P(t_2)$ is the probability of seeing token $t_2$ in the corpus. In other words, we need to calculate the likelihoods for all token pairs in our current token pairs and pick the one with the highest likelihood. While we could calculate all three probabilities to get the likelihood, let's see how we can simplify this formula. First, we can apply the formula for conditional probabilities $P(B|A) = P(B,A)P(A)$ for two random events $A$ and $B$. With $A$ and $B$ being our two tokens $t_1$ and $t_2$, we can write:\n",
    "\n",
    "$$\\large\n",
    "\\frac{P(t_1, t_2)}{P(t_1) P(t_2)} = \\frac{P(t_2|t_1)P(t_1)}{P(t_1) P(t_2)} = \\frac{P(t_2|t_1)}{P(t_2)}\n",
    "$$\n",
    "\n",
    "In terms of n-gram language models, $P(t_2)$ is the unigram probability for token $t_2$, and $P(t_2|t_1)$ is the bigram probability for the token sequence/pair $(t_2,t_1)$. We can therefore calculate both probabilities as follows:\n",
    "\n",
    "$$\\large\n",
    "P(t_2) = \\frac{count(t_2)}{N}\\ , \\quad P(t_2|t_1) = \\frac{count(t_1,t_2)}{count(t_1)}\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of tokens in the current corpus state, $count(t_1,t_2$ is the total number of occurrences of token pair $(t_1,t_2)$, $count(t_1)$ is the total number of occurrences of token $t_1$, and $count(t_1)$ is the total number of occurrences of token $t_2$. Plugging these term into our initial formula, we get:\n",
    "\n",
    "$$\\large\n",
    "\\frac{P(t_1, t_2)}{P(t_1) P(t_2)} = \\frac{P(t_2|t_1)}{P(t_2)} = \\frac{\\frac{count(t_1,t_2)}{count(t_1)}}{\\frac{count(t_2)}{N}} = \\frac{N\\cdot count(t_1,t_2)}{count(t_1)\\cdot count(t_2)}\n",
    "$$\n",
    "\n",
    "Thus, compared to BPE, we not only need to calculate the number of occurrences of each token pair $(t_1, t_2)$, but also the number of occurrences of the two component tokens $t_1$ and $t_2$. As a last little improvement, consider that we are not really interested in the exact likelihood values, but only want to find the token pair with the largest likelihood. Notice that the number of tokens $N$ does not depend on a current token pair, and therefore does not affect the final ranking of token pairs. This allows us to simply ignore $N$ in the calculation. Of course this means that the likelihood is not only proportional to the final formula\n",
    "\n",
    "$$\\large\n",
    "\\frac{P(t_1, t_2)}{P(t_1) P(t_2)} \\propto \\frac{count(t_1,t_2)}{count(t_1)\\cdot count(t_2)}\n",
    "$$\n",
    "\n",
    "The method `find_best_token_pair()` in the code cell below implements this calculation by iterating over all token sequences in the corpus state, and for each sequence, iterating over all to sum up the total number of occurrences of each token pair and each individual token. The method then calculates the score for each token pair, where the score is proportional to the pair's likelihood and calculated by the formula given above. The method finally returns the token pair with the highest (any ties are broken randomly). The method also returns a dictionary with all token pairs and their respective scores; this is only to have a look at the scores and not need for the algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700759dd-4d88-4cb0-a78c-178ebe7c7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_token_pair(corpus_state):\n",
    "    # Initialize dictionaries to keep track of all counts\n",
    "    token_counts = collections.defaultdict(int)\n",
    "    token_pair_counts = collections.defaultdict(int)\n",
    "    # Iterate over all token sequences in the current corpus state\n",
    "    for word, freq in corpus_state.items():\n",
    "        sequence = word.split()\n",
    "        # Special case: the sequence already a single token\n",
    "        if len(sequence) == 1:\n",
    "            token_counts[sequence[0]] += freq\n",
    "            continue\n",
    "        # Iterate over all token pair and update the count\n",
    "        for i in range(len(sequence)-1):\n",
    "            pair = (sequence[i], sequence[i+1])\n",
    "            token_counts[f\"{sequence[i]}\"] += freq\n",
    "            token_pair_counts[pair] += freq\n",
    "        # Don't forget the last token that was not captured by the previous loop\n",
    "        token_counts[sequence[-1]] += freq\n",
    "    # Calculate the score of all token pairs using the formula approximating the likelihood\n",
    "    token_pair_scores = { \n",
    "        ' '.join(pair): count / (token_counts[pair[0]] * token_counts[pair[1]]) \n",
    "        for pair, count in token_pair_counts.items() \n",
    "    }\n",
    "    # Return the most frequent pair (if their are ties, we just randomly break them) + all token pair counts\n",
    "    return max(token_pair_scores.keys(), key=(lambda key: token_pair_scores[key])), token_pair_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485342a-cb0a-42e1-b1b8-809177de48bc",
   "metadata": {},
   "source": [
    "Using the method `find_best_token_pair()`, we can identify the first token pair to merge for our initial corpus state. In general, there might be multiple token pairs that have the same highest number of occurrences. In this situation, we pick any token pair from this subset randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2444738d-b01b-45af-92c0-926f180dc084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-initialize the corpus state and the vocabulary to ensure a consistent output of this code cell\n",
    "corpus_state, vocabulary = initialize_corpus_state([doc])\n",
    "\n",
    "#top_token_pair, token_pair_counts = find_most_frequent_token_pair(corpus_state)\n",
    "top_token_pair, token_pair_scores = find_best_token_pair(corpus_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cfeac-8964-486e-a369-5f6d92877881",
   "metadata": {},
   "source": [
    "Let's first look at the counts for all token pairs. Again, this is just for illustrative purposes and not required for the WordPiece algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b844f37-06ba-4b29-989f-6e8943e30b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"l _o\": 0.125,\n",
      "  \"_o _w\": 0.0673076923076923,\n",
      "  \"_w _e\": 0.03418803418803419,\n",
      "  \"_e _r\": 0.05555555555555555,\n",
      "  \"n _e\": 0.05555555555555555,\n",
      "  \"_e _w\": 0.02564102564102564,\n",
      "  \"_e _s\": 0.05555555555555555,\n",
      "  \"_s _t\": 0.1111111111111111,\n",
      "  \"w _i\": 0.3333333333333333,\n",
      "  \"_i _d\": 0.3333333333333333,\n",
      "  \"_d _e\": 0.05555555555555555,\n",
      "  \"_o _n\": 0.125,\n",
      "  \"_n _g\": 1.0,\n",
      "  \"_g _e\": 0.05555555555555555\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(token_pair_scores, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806ecd1-069c-483b-8bc9-88276b665e79",
   "metadata": {},
   "source": [
    "We can see that the token pair that has the highest score is : \"_n _g\", so this is the pair that will be merged next. Let's see `find_best_token_pair() has indeed returned this token pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be278529-c093-4a83-bef4-0cf78a3c29ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_n _g\n"
     ]
    }
   ],
   "source": [
    "print(top_token_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4ee22-a0e6-4f77-acb1-eec2dc377e2a",
   "metadata": {},
   "source": [
    "In the following, let's assume the most frequent token pair that was returned is *\"_n _g\"*.\n",
    "\n",
    "##### Perform Merge\n",
    "\n",
    "With the next best token pair found, we can actually perform the merging step. This step includes three core substeps:\n",
    "\n",
    "* Merge the token pair into a new token (e.g., *\"_n _g\"* becomes *\"_ng\"*)\n",
    "* Add this new token (here, *\"_ng\"*) to the vocabulary\n",
    "* Update the corpus state by replacing all occurrences of the token pair with the new token &mdash; for our example, replace all occurrences of *\"_n _g\"* in the corpus state with *\"_ng\"*.\n",
    "\n",
    "The first step is very straightforward. To merge a token pair, we remove the CTOKEN character from the second token &mdash; and the second token will always have this special token &mdash; and concatenate it with the first token. The first token may or may not have the CTOKEN at the beginning. For convenience the method `create_new_token()` wraps up this first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4be3ce-9ecd-4768-99e4-225ad4bde6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng\n",
      "_ng\n"
     ]
    }
   ],
   "source": [
    "def create_new_token(token_pair):\n",
    "    t1, t2 = token_pair.split()\n",
    "    return ''.join([t1, re.sub(CTOKEN, \"\", t2)])\n",
    "\n",
    "print(create_new_token(\"n _g\"))\n",
    "print(create_new_token(\"_n _g\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce5997-1f9a-4f28-bf8c-a823a9c0af2c",
   "metadata": {},
   "source": [
    "\n",
    "The method `perform_merge()` implements these three substeps; using the method `create_new_token()` for Step 1. Updating the corpus state is only a bit more complex. Since the corpus state is implemented as a dictionary and replacing all occurrences of the token pair with the merge token means changing the keys of this dictionary, we cannot simply iterate over this dictionary and directly change the keys. We therefore break this update step up into two loops\n",
    "\n",
    "* In the first loop, we find all keys (i.e., the sequences in the corpus state) that contain the token pair (at least once) and therefore need to be updated. We identify these keys using a Regular Expression that matches the token pair in a key. For example, given the key *\"l _o _n _g _e _r\"*, the Regular Expression looking for *\"_n _g\"* would match; but would not match for *\"n _e _w _e _s _t\"*. We keep track of all matching keys using the dictionary `matches` where its keys are the old keys in corpus state and the values are the new keys for the corpus state. We use the same Regular Expression to generate the new keys; for example, *\"l _o _n _g _e _r\"* becomes *\"l _o _ng _e _r\"*.\n",
    "\n",
    "* In the second loop, we iterate over all matches to add the new keys to the corpus state and give them the values of their respective old keys (those values representing the initial count do not change). We do this using the built-in `pop()` method which automatically removes the entry for the old key from the corpus state.\n",
    "\n",
    "Finally we return the merge (i.e., the tuple of the token pair and the new token &mdash; we need this later), the updated corpus state, and the updated vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ab06b95-0369-442d-9bb3-6564a36be438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_merge(token_pair, corpus_state, vocabulary):\n",
    "    # Create new token by merging token pair\n",
    "    new_token = create_new_token(token_pair)\n",
    "    # Create merge as tuple of token pair and new token\n",
    "    merge = (token_pair, new_token)\n",
    "    # Add new token to vocabulary\n",
    "    vocabulary.add(new_token)\n",
    "    # Define search pattern\n",
    "    pattern = re.compile(r\"(?<!\\S)\" + re.escape(token_pair) + r\"(?!\\S)\")\n",
    "    # Loop through corpus state and record which keys/sequences need to be updated\n",
    "    matches = {}\n",
    "    for sequence, count in corpus_state.items():\n",
    "        for match in pattern.finditer(sequence):\n",
    "            matches[sequence] = pattern.sub(new_token, sequence)\n",
    "    # Perform the update of keys/sequences\n",
    "    for old, new in matches.items():\n",
    "        corpus_state[new] = corpus_state.pop(old)\n",
    "    # Return the updated corpus state and vocabulary\n",
    "    return merge, corpus_state, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517848a-3ad9-44cb-956b-dc5033eb1403",
   "metadata": {},
   "source": [
    "For testing, we can run the method `perform_merge()` over our initial corpus state and vocabulary. We only re-initialize the both corpus state and vocabulary to ensure the output of the code cell is always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "062d1335-15a7-4bff-bdcc-a95916699afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-initialize the corpus state and the vocabulary to ensure a consistent output of this code cell\n",
    "corpus_state, vocabulary = initialize_corpus_state([doc])\n",
    "\n",
    "merge, corpus_state, vocabulary = perform_merge('_n _g', corpus_state, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39734293-1be1-425f-b12a-2a93c77f696a",
   "metadata": {},
   "source": [
    "Let's have a look at all three return values of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac71885-10be-4128-a067-8c9af9c449bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('_n _g', '_ng')\n",
      "\n",
      "Updated corpus state:\n",
      "{\n",
      "  \"l _o _w\": 5,\n",
      "  \"l _o _w _e _r\": 2,\n",
      "  \"n _e _w _e _s _t\": 6,\n",
      "  \"w _i _d _e _s _t\": 3,\n",
      "  \"l _o _ng _e _r\": 1\n",
      "}\n",
      "\n",
      "Updated vocabulary\n",
      "{'_o', '_e', '_r', '_w', '_i', '_g', 'l', 'w', '_t', '_ng', '_s', '_d', 'n', '_n'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Merge: {merge}\")\n",
    "print()\n",
    "print(\"Updated corpus state:\")\n",
    "print(json.dumps(corpus_state, indent=2))\n",
    "print()\n",
    "print(\"Updated vocabulary\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b445b6b-10f4-43e1-9b73-8bd53417d7ee",
   "metadata": {},
   "source": [
    "Particularly, notice the changes in the corpus state with *\"l _o _n _g _e _r\"* becoming *\"l _o _ng _e _r\"*. Also, *\"_ng\"* has been added to the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc34b75-19d6-482c-8e60-4d27c08f6683",
   "metadata": {},
   "source": [
    "### WordPiece vs. BPE &mdash; Intuition\n",
    "\n",
    "Since WordPiece and BPE fundamentally only differ with respect to how the next token pair for merging is determined, it is a good time to try to get some intuition behind that difference. Recall the the score for a token pair $(t_1, t_2)$ in WordPiece is computed as:\n",
    "\n",
    "$$\\large\n",
    "\\frac{count(t_1,t_2)}{count(t_1)\\cdot count(t_2)}\n",
    "$$\n",
    "\n",
    "In contrast, BPE only uses $count(t_1,t_2)$! This means that WordPiece will favor a token pair even if it is *\"not\"* the most frequent. More specifically, WordPiece favors token pairs where the two tokens, if they appear in a word, very often appear together. For example, given our toy dataset, the token pair (*\"_n\"*, *\"_g\"*) appears only once in the corpus state, while, e.g., (*\"_e\"*, *\"_s\"*) nine times (and would therefore be chosen by BPE). However, both tokens *\"_n\"* and *\"_g\"* happen to appear only once. In simple terms, this means that when we see the token *\"_n\"* we are very likely see it together with token *\"_g\"*, and vice versa. In other words, the higher the score of a token pair, the more frequently both tokens co-occur than would be expected under an assumption of both tokens being independent.\n",
    "\n",
    "Or just by dissecting the formula above, the score is high if the numerator (the count of the token pair) is large and the denominator (the product of the two token counts) is small. Of course, the following two inequalities $count(t_1,t_2) \\leq count(t_1)$ and $count(t_1,t_2) \\leq count(t_1)$ always hold. Thus, the maximum score for a token pair is $1$. Therefore, for the denominator to be as small as possible, the two tokens of a token pair may have as few other token pairs as possible (or as an individual). The more often one or both tokens appear \"outside\" that pair, the lower the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb7fca-de79-4d76-ab70-fb40e9ec2311",
   "metadata": {},
   "source": [
    "### WordPiece Learning Algorithm\n",
    "\n",
    "With these three core steps, that is:\n",
    "* Initialization of the corpus state and vocabulary\n",
    "* Finding the best token pair (for the current corpus state), and\n",
    "* Performing the merging step and updating the corpus state\n",
    "\n",
    "We now have everything in place to implement the WordPiece learning algorithm by plugging the methods implementing those three steps together; see the method `wordpiece_learn()` in the code cell below. Although the implementation of this method is rather straightforward, three small details are worth mentioning\n",
    "\n",
    "* One of the aforementioned advantages of WordPiece is that we can specify the maximum size of the resulting vocabulary. Since each merging step adds a new token to the vocabulary, we can restrict the size of the vocabulary by limiting the number of merging steps. However, since our initial vocabulary is not empty but the set of all unique characters, the number of merging steps &mdash; that is, the number of iterations `num_iter` in the code &mdash; derives from the difference of the specified maximum vocabulary size `max_vocab_size` and the size of the initial vocabulary.\n",
    "\n",
    "* In principle, particularly if the corpus is not very large, the algorithm might perform more merges than possible. This happens when all words in the corpus state have been merged to their original form. For example, after a certain amount of iterations, say, *\"n _e _w _e _s _t\"* will have been merged to \"newest\". If this is true for all words in the corpus state, the corpus state does no longer contain any pair of tokens. In this case, the method `find_best_token_pair()` will throw an error and exit the loop since the learning algorithm has finished.\n",
    "\n",
    "* In each iteration, we keep track of the recent merge by adding it to a list of all previous merges. This list of merges is in fact the most important return value of the learning algorithm as it is used for tokenizing text. This includes that the order matters as we want to merge tokens in a new text in the same order as we merged them during the learning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef36d151-02d4-4e5c-a0f6-9baab3b5ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordpiece_learn(corpus, max_vocab_size=10000):\n",
    "\n",
    "    # Initialize corpus state and vocabulary\n",
    "    corpus_state, vocabulary = initialize_corpus_state(corpus)\n",
    "\n",
    "    # Initialize the list of merges\n",
    "    merges = []\n",
    "\n",
    "    # Calculate the number of merging steps to ensure the maximum size of the vocabulary\n",
    "    num_iter = max_vocab_size - len(vocabulary)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "\n",
    "        # Find the most frequent pair; if this fails, no more merging was possible and we can stop\n",
    "        try:\n",
    "            top_token_pair, _ = find_best_token_pair(corpus_state)\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        # Update corpus state and the vocabulary\n",
    "        merge, corpus_state, vocabulary = perform_merge(top_token_pair, corpus_state, vocabulary)\n",
    "        \n",
    "        # Add newly merged token to vocabulary\n",
    "        merges.append(merge)\n",
    "\n",
    "    # Return list of merges, the corpus state, and the vocabulary\n",
    "    return merges, corpus_state, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54029dd-c1e9-4367-953e-fd4dc35263a9",
   "metadata": {},
   "source": [
    "Let's train a WordPiece tokenizer over our toy document.\n",
    "\n",
    "**Your turn:** Try different values for `max_vocab_size` and inspect the results. Of course, the larger this value, the larger the final vocabulary and the list of merges, but also the corpus state shows larger tokens. For example, a large value, say, `max_vocab_size=1000`, the learning algorithm tries to make more merging steps as actually possible. You can tell by looking at the corpus state where all words are merged into a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a4288b-b420-459c-8950-aebbc55b90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final corpus state:\n",
      "{\n",
      "  \"low\": 5,\n",
      "  \"longer\": 1,\n",
      "  \"lower\": 2,\n",
      "  \"widest\": 3,\n",
      "  \"newest\": 6\n",
      "}\n",
      "\n",
      "Final vocabulary (size: 30):\n",
      "{'_r', '_g', 'newest', 'ne', 'widest', '_s', 'wide', 'new', '_ng', '_i', '_w', 'wi', 'long', 'w', '_er', 'lo', 'longe', 'n', '_o', '_e', 'low', 'longer', '_t', 'newe', 'wid', '_n', 'l', '_d', 'lower', '_st'}\n",
      " \n",
      "Final list of merges:\n",
      "[('_n _g', '_ng'), ('w _i', 'wi'), ('wi _d', 'wid'), ('l _o', 'lo'), ('lo _ng', 'long'), ('_s _t', '_st'), ('lo _w', 'low'), ('long _e', 'longe'), ('longe _r', 'longer'), ('n _e', 'ne'), ('ne _w', 'new'), ('wid _e', 'wide'), ('_e _r', '_er'), ('new _e', 'newe'), ('low _er', 'lower'), ('wide _st', 'widest'), ('newe _st', 'newest')]\n"
     ]
    }
   ],
   "source": [
    "merges, corpus_state, vocabulary = wordpiece_learn([doc], max_vocab_size=100)\n",
    "\n",
    "print(f\"Final corpus state:\\n{json.dumps(corpus_state, indent=2)}\\n\")\n",
    "print(f\"Final vocabulary (size: {len(vocabulary)}):\\n{vocabulary}\\n \")\n",
    "print(f\"Final list of merges:\\n{merges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4345f-649e-4a91-ad4c-205c98fc52cf",
   "metadata": {},
   "source": [
    "### WordPiece Tokenization Algorithm\n",
    "\n",
    "Once we applied the WordPiece learning algorithm over a corpus, using this learned model for actually tokenizing an arbitrary text is very straightforward. In fact, strictly speaking, we only need the list of merges that was returned from the learning algorithm (see above). In some sense, the tokenization algorithm mimics the learning algorithm. This includes that we first perform pretokenization and treat each initial token separately. The method `tokenize_word()` tokenizes a single (initial) token based on the learned list of merges as follows:\n",
    "\n",
    "* First, the method splits the word into character tokens and adds the `CTOKEN` token using the `generate_sequence()` method. For example, the words *\"newer\"* becomes *\"n _e _w _e _r\"*\n",
    "\n",
    "* Then, the method iterates over the list of merges and checks if a merge can be applied, and does so if a match is found. For example, the merge `(\"_e _r\", \"_er\")` will find a match in *\"n _e _w _e _r\"* and convert it the *\"n _e _w _er\"*. To find the matches and perform the merges, we use the same Regular Expression we have already seen in the learning algorithm.\n",
    "\n",
    "The rest of the implementation is merely for printing the intermediate results for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96a85289-1d79-438f-ac62-e881d1d40ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(word, merges, verbose=False):\n",
    "    sequence = generate_sequence(word)\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(sequence)\n",
    "        \n",
    "    for p, m in merges:\n",
    "        if p not in sequence:\n",
    "            continue\n",
    "            \n",
    "        p = re.compile(r'(?<!\\S)' + re.escape(p) + r'(?!\\S)')\n",
    "        sequence = p.sub(m, sequence)\n",
    "        \n",
    "        if verbose == True:\n",
    "            print(sequence)\n",
    "        \n",
    "    return sequence.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8467166b-18c6-41ff-9ee0-0428561062cb",
   "metadata": {},
   "source": [
    "We can now run the `tokenize_word()` over a word that was not in the training document. Of course, the exact output will depend on the value for `max_vocab_size` you chose to train the WordPiece tokenizer. For example, with `max_vocab_size=0`, the word will be split into its individual characters since a new merge is performed. In other words, with `max_vocab_size=0`, the WordPiece tokenizer becomes a character tokenizer. In contrast, if the value for `max_vocab_size` is very large, the WordPiece tokenizer is more likely to behave like a word tokenizer.\n",
    "\n",
    "**Your turn:** Run the method `wordpiece_learn()` implementing the WordPiece learning algorithm with different values for `max_vocab_size` and see how the output of the code cell below changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6ed2cf-d51a-4b69-aea3-7c42a70e956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n _e _w _e _r\n",
      "ne _w _e _r\n",
      "new _e _r\n",
      "new _er\n",
      "new _er\n",
      "['new', '_er']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_word('newer', merges, verbose=True)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81e891-3b1b-445a-adae-7f7f2d4051e0",
   "metadata": {},
   "source": [
    "To tokenize a complete document &mdash; again, mimicking the learning algorithm &mdash; we first need to pretokenize the document, and then run the method `tokenize_word()` over each initial token. The method `tokenize` implements these basic steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e26bff-11a1-4521-b35c-39f95153167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc, merges, verbose=False):\n",
    "    pretokens = pretokenize(doc)\n",
    "\n",
    "    tokens = []\n",
    "    for pt in pretokens:\n",
    "        tokens.extend(tokenize_word(pt, merges, verbose=verbose))\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b5865-9841-411f-b8f8-2f6eebaa6d95",
   "metadata": {},
   "source": [
    "The code cell below defines another example document to test the behavior of method `tokenize()`. As before, the exact output will depend on the value of `max_vocab_size` when training the tokenizer and the document `doc` itself. Feel free to modify the document by adding new words or tweaking existing ones. You are also encouraged to run the code cell with different versions of the tokenizer (i.e., trained using different values for `max_vocab_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e97e941-72c8-4fce-a5c6-9c64b84abf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n _e _w _e _r\n",
      "ne _w _e _r\n",
      "new _e _r\n",
      "new _er\n",
      "new _er\n",
      "l _o _n _g _e _s _t\n",
      "l _o _ng _e _s _t\n",
      "lo _ng _e _s _t\n",
      "long _e _s _t\n",
      "long _e _st\n",
      "longe _st\n",
      "k _n _e _w\n",
      "k _n _e _w\n",
      "i _n _g _e _s _t\n",
      "i _ng _e _s _t\n",
      "i _ng _e _st\n",
      "b _e _l _o _n _g\n",
      "b _e _l _o _ng\n",
      "b _e _l _o _ng\n",
      "n _e _w _e _s _t\n",
      "n _e _w _e _st\n",
      "ne _w _e _st\n",
      "new _e _st\n",
      "newe _st\n",
      "newest\n"
     ]
    }
   ],
   "source": [
    "doc2 = \"newer longest knew ingest belong newest\"\n",
    "\n",
    "example_token_list = tokenize(doc2, merges, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ce3ed3c-db99-429d-abf1-a50b56f0e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', '_er', 'longe', '_st', 'k', '_n', '_e', '_w', 'i', '_ng', '_e', '_st', 'b', '_e', '_l', '_o', '_ng', 'newest']\n"
     ]
    }
   ],
   "source": [
    "print(example_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c07df-0b76-4a43-9942-d796b370656c",
   "metadata": {},
   "source": [
    "### Detokenize\n",
    "\n",
    "If we would use the WordPiece tokenizer only to tokenize a text to serve as input for a machine learning model, we could stop here. However, text generation tasks such as machine translation, question answering, chatbots, etc. not only take tokenized text as input but also generate text in the form of tokens from the learned vocabulary. This means we need ways to convert a list of tokens back to a proper text. However, at least in its basic form, can be done performing to simple steps:\n",
    "\n",
    "* Concatenate all tokens into a single string, and\n",
    "* Remove all strings containing a whitespace character followed by a special CTOKEN character(s).\n",
    "\n",
    "The method `detokenize()` implements these two trivial steps; and let's test it on some example token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "285fc98a-ea34-4ed1-8ed9-4795d1b752c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newer longest knew ingest belong newest\n"
     ]
    }
   ],
   "source": [
    "def detokenize(tokens: list):\n",
    "    doc = ' '.join(tokens)\n",
    "    return re.sub(f\" {CTOKEN}\", \"\", doc).strip()\n",
    "\n",
    "print(detokenize(example_token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46307c-67fb-4984-9659-71c40eee2c4b",
   "metadata": {},
   "source": [
    "This step of detokenizing a list of tokens to a string actually shows why we need the special CTOKEN character. Without it, we could not tell &mdash; at least not easily and reliably &mdash; which tokens should be merged to form a word in the output string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562dfee3-1f2f-4226-859c-a0655032823f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de0d73-a18a-44f1-9358-f574c8111418",
   "metadata": {},
   "source": [
    "## Discussion & Limitations\n",
    "\n",
    "**Representation of tokens:** In our implementation, the tokens in the corpus state are represented as their actual strings of characters. The advantage is that it is much easier to follow how the algorithm works. Practical applications, however, commonly represent the tokens as unique ids, and the vocabulary maintains a mapping between the ids and their respective tokens. For example, instead of representing and entry in the corpus state like\n",
    "\n",
    "```\n",
    "{\n",
    "  ...\n",
    "  \"wid _e _st\": 3,\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "the alternative representation using ids could look like\n",
    "\n",
    "```\n",
    "{\n",
    "  ...\n",
    "  \"230 4 108\": 3,\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "Where $230$ maps to *\"wid\"*, $4$ to *\"_e\"*, and $108$ to *\"_st\"*. Thus, every time a token pair gets merged into a new token, a new id gets created for that token. This approach has a couple advantages. Firstly, the implementation in terms of memory management gets easier since integer values have a fixed size in bytes, whereas string tokens vary in size during the learning when the token gets merged. And secondly, token ids are the \"natural\" input for most machine learning algorithms, incl. neural networks. It is therefore more efficient if the tokenizer directly outputs a list of ids. If needed, ids can always be decoded using the mapping between ids and string tokens maintained in the vocabulary.\n",
    "\n",
    "**Special characters:** For our example implementation we used the underscore `_` as a special character to mark the continuation of a word. We saw that this was needed to reconstruct a list of tokens into a proper text. We already mentioned that the choice of `_` was simply to ease the representation, but we had to make the assumption that the training data does not contain underscores. In real-world text corpora, of course, underscores might very well occur. Therefore, practical WordPiece tokenizer algorithms favor characters that are arbitrarily unlikely to appear in a text document. For example, the common BERT tokenizer uses **##** as a special character (sequence) for the tokenizer; whether a single character or a sequence of characters is used to mark the continuation of a word does not affect the algorithm.\n",
    "\n",
    "**Smart(er) pretokenization:** WordPiece &mdash; as well as most other subword tokenization algorithms &mdash; requires a pretokenization to an initial list of tokens to initialize the corpus state. For English and many other languages, doing this by breaking up a text with respect to whitespace characters, is a quick and simple approach &mdash; and it works, as we have seen throughout the notebook. However, it is very common in English that there is no whitespace between tokens of different categories. For example, there is no whitespace before punctuation marks, and no whitespace before/after a closing/opening parenthesis or quote character. Simple whitespace pretokenization therefore yields initial tokens that do not \"belong together\". For example, an initial corpus state might look like this:\n",
    "\n",
    "```\n",
    "{\n",
    "  ...\n",
    "  \"w _i _d _e _s _t _.\": 30,\n",
    "  \"w _i _d _e _s _t _?\": 16,\n",
    "  \"w _i _d _e _s _t _!\": 9,\n",
    "  \"w _i _d _e _s _t _,\": 3,\n",
    "  \"w _i _d _e _s _t _:\": 5,\n",
    "  \"w _i _d _e _s _t _;\": 10,\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "While, in principle, WordPiece still works, it might lead to suboptimal allocation of limited vocabulary slots. For example, with this corpus state, both *\"_st?\"* and *\"_st!\"* (and maybe others) might make it into the vocabulary although both tokens are from the perspective of a word they are the same. and model capacity. To avoid this, practical WordPiece implementations use some smarter pretokenization to the learning algorithm from merging across character categories. For example, with a pretokenizer that splits words from punctuation marks, our corpus state from above might look as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "  ...\n",
    "  \"w _i _d _e _s _t\": 73,\n",
    "  \".\": 10085,\n",
    "  \"?\": 3120,\n",
    "  \"!\": 5467,\n",
    "  \",\": 8985,\n",
    "  \":\": 1403,\n",
    "  \";\": 2050,\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "To give a concrete example, GPT2 used the following Regular Expression the pretokenize input texts (note: the expression has be slightly adapted to fit its use in this notebook, and later GPT uses more revised expression; however, here it's only used to show an alternative to naive whitespace pretokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e2845fb-ced6-4125-8632-d6d27adc0b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", 'world', '123', 'how', \"'s\", 'are', 'you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "gpt2pattern = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|\\p{L}+|\\p{N}+|[^\\s\\p{L}\\p{N}]+\"\"\")\n",
    "\n",
    "print(regex.findall(gpt2pattern, \"Hello've world123 how's     are you!!!?   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303c4b2-cf6b-44f9-baca-25954f03787f",
   "metadata": {},
   "source": [
    "From the output of the previous code cell, you can already see how the Regular Expression is working; in simple terms it splits an input text into tokens that are:\n",
    "\n",
    "* from a predefined set of clitics (*\"'s\"*, *\"'t\"*, *\"'re\"*, *\"'ve\"*, *\"'m\"*, *\"'ll\"* ,*\"'d\"*)\n",
    "* a sequence of letters of arbitrary length\n",
    "* a sequence of digits of arbitrary lengths\n",
    "* a sequence of anything but letter, digits, and whitespaces of arbitrary length\n",
    "\n",
    "It is obvious that this Regular Expression can be modified to potentially improve the pretokenization step further. However, none of those additions change the fundamental WordPiece learning and tokenization algorithm covered in this notebook. In fact, many of these improvements you are likely to add yourself to the basic algorithm covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763d2e5-ad29-4956-9577-faf6dab56b1d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a4504-5230-4127-89d0-93fdbb46844f",
   "metadata": {},
   "source": [
    "## Example Application\n",
    "\n",
    "So far, we only run our WordPiece tokenizer implementation only over a very simple and \"artificial\" example document to better understand all the steps of the algorithm. Now let's use a larger document to see how our tokenizer performs. While in practice, huge corpora are used to train a subword tokenizer such as WordPiece, here we limit ourselves to a single book to keep the training time in check. The result will still give us very interesting insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9c6aa-1a7a-45aa-86d3-193a8450de2b",
   "metadata": {},
   "source": [
    "### Revised WordPiece Tokenizer Implementation\n",
    "\n",
    "For this application use case, we provide the class `MyWordPieceTokenizer` in the file `src/tokenizer.py`. This class contains exactly the methods we used so far to train and use our tokenizer. However, incorporating all methods into this class allows for a cleaner code and a much easier usage &mdash; now that we understand how WordPiece works. This means, we can train our WordPiece tokenizer now with a single line of code. The code for the class contains only two minor changes:\n",
    "\n",
    "* Instead of the underscore character to mach the continuation of a word &mdash; which kept the illustrations how the algorithm works cleaner &mdash; we now use `##` for this purpose (like the BERT tokenizer based on WordPiece)\n",
    "* For pretokenization, it supports naive whitespace tokenization as well as approach done by GPT2 (see above).\n",
    "\n",
    "Let's first do this for our example document before using the real-world document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9afa70ad-848f-44ea-b027-b6afe60cc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilize corpus and vocabulary...\n",
      "Perform 87 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                                       | 17/87 [00:00<00:00, 4181.76it/s]\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer_example = MyWordPieceTokenizer(pretokenize=MyWordPieceTokenizer.PRE_TOKENIZE__SPLIT).fit([doc], max_vocab_size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c530d-e84c-4324-8ae8-0a0887036ee8",
   "metadata": {},
   "source": [
    "The progress bar will stop before 100% if the value for `max_vocab_size` is large enough so that the loop will stop before the expected number of iterations has been reached. Recall, this happens if all possible token pairs have been merged, and therefore no further merge is possible. And this will happen very quickly with very small documents like our toy document.\n",
    "\n",
    "As this is our small toy document, we can still look at the final corpus state, vocabulary and the list of merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f71654f5-6a0e-440c-b7d5-820e364c38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final corpus state:\n",
      "{\n",
      "  \"low\": 5,\n",
      "  \"longer\": 1,\n",
      "  \"lower\": 2,\n",
      "  \"widest\": 3,\n",
      "  \"newest\": 6\n",
      "}\n",
      "\n",
      "Final vocabulary (size: 30):\n",
      "{'##g', 'newest', '##st', 'ne', 'widest', 'wide', 'new', 'wi', 'long', 'w', 'lo', '##ng', 'longe', '##o', 'n', '##w', '##n', 'low', 'longer', '##r', '##t', 'newe', '##er', '##i', '##e', 'wid', '##d', 'l', '##s', 'lower'}\n",
      " \n",
      "Final list of merges:\n",
      "[('##n ##g', '##ng'), ('w ##i', 'wi'), ('wi ##d', 'wid'), ('l ##o', 'lo'), ('lo ##ng', 'long'), ('##s ##t', '##st'), ('lo ##w', 'low'), ('long ##e', 'longe'), ('longe ##r', 'longer'), ('n ##e', 'ne'), ('ne ##w', 'new'), ('wid ##e', 'wide'), ('##e ##r', '##er'), ('new ##e', 'newe'), ('low ##er', 'lower'), ('wide ##st', 'widest'), ('newe ##st', 'newest')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final corpus state:\\n{json.dumps(my_tokenizer_example._corpus_state, indent=2)}\\n\")\n",
    "print(f\"Final vocabulary (size: {len(my_tokenizer_example._vocabulary)}):\\n{my_tokenizer_example._vocabulary}\\n \")\n",
    "print(f\"Final list of merges:\\n{my_tokenizer_example._merges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb5ca5-3928-46c7-9af2-44dca7655988",
   "metadata": {},
   "source": [
    "Of course, when using the same value for `max_vocab_size`, the result should be exactly the same as seen before &mdash; apart from the different special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032e574-5cd0-4e0b-bd48-962e08ca3f6d",
   "metadata": {},
   "source": [
    "### Training using Real-World Data\n",
    "\n",
    "For the training we will use content from [Project Gutenberg](https://www.gutenberg.org/). Project Gutenberg is a digital library that provides free access to a vast collection of public domain books and literary works. Founded by Michael S. Hart in 1971, it is one of the oldest digital libraries in existence, aiming to democratize access to literature and knowledge. The project offers over 60,000 eBooks, including classic novels, historical documents, and reference works, in a variety of formats such as plain text, HTML, and ePub, making them accessible across different devices. The initiative relies heavily on volunteers to digitize, proofread, and maintain its collection, ensuring that these works are preserved and made universally available. Since it focuses on texts that are no longer under copyright protection, Project Gutenberg plays a key role in keeping timeless literary and cultural works accessible to the public for free, fostering education and literacy worldwide.\n",
    "\n",
    "The book of choice is [*Treasure Island*](https://www.gutenberg.org/ebooks/120) by Robert Louis Stevenson. It is a classic adventure novel that tells the story of young Jim Hawkins and his journey to uncover buried pirate treasure. The tale begins when Jim discovers a mysterious map among the belongings of a deceased sailor at his familys inn. The map leads to a hidden treasure on a distant island, and Jim joins an expedition to retrieve it, led by the noble Dr. Livesey and the eccentric Squire Trelawney. As the voyage unfolds, Jim realizes that not all the crew members can be trusted, particularly the cunning and charismatic Long John Silver, a one-legged cook with his own secret agenda. The story is packed with thrilling battles, daring escapes, and moments of betrayal and bravery, as Jim and his allies face off against mutinous pirates. Treasure Island is a timeless tale of adventure, exploration, and the moral complexities of greed and loyalty.\n",
    "\n",
    "Let's first read the file into the variable `book`.\n",
    "\n",
    "**Your turn:** You can download other/more materials from the Project Gutenberg website to expand the overall training corpus. Further down below, when you look at some example sentences that have been tokenized using our trained WordPiece tokenizer, you will notice some limitations when the training dataset is not large and diverse enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08f2f102-130e-48c9-be8a-42b0485ec045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 375911\n"
     ]
    }
   ],
   "source": [
    "with open(treasure_island_book, \"r\") as file:\n",
    "    book = file.read().replace('\\n', '').strip()\n",
    "\n",
    "print(f\"Number of characters: {len(book)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab662241-c8fa-4400-b020-d8039c03a9e9",
   "metadata": {},
   "source": [
    "Using our own WordPiece tokenizer implementation, we can now run the learning algorithm using *Treasure Island*. Feel free to modify the pretokenization approach and maximum vocabulary size. Keep in mind that our implementation is not optimized for performance and this is not a tiny toy document. As such, the learning will take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce504767-5d82-48ea-9833-3ca96f2f2dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilize corpus and vocabulary...\n",
      "Perform 19839 iterations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19839/19839 [15:31<00:00, 21.29it/s]\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer_book = MyWordPieceTokenizer(pretokenize=MyWordPieceTokenizer.PRE_TOKENIZE__GPT2).fit([book], max_vocab_size=20000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c2355-b786-4d12-81dc-c52c6d2db4ff",
   "metadata": {},
   "source": [
    "Let's tokenize a couple of example sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03c50feb-5435-494a-9fcc-73010a540bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Th', '##e', '##r', '##e', 'is', 'still', 'a', 'lot', 'of', 'treasur', '##e', 'buried', 'on', 'th', '##e', 'island', '.']\n",
      "['I', \"'\", '##ve', 'check', '##e', '##d', ',', 'my', 'last', 'shipm', '##e', '##nt', 'was', 'd', '##e', '##lay', '##e', '##d', '.']\n",
      "['Th', '##e', 'captain', 'and', 'th', '##e', 'li', '##e', '##ut', '##e', '##nant', 'had', 'a', 'discussion', '.']\n",
      "['Th', '##e', 't', '##e', '##am', 'm', '##emb', '##e', '##rs', 'a', '##r', '##e', 'Al', '##ic', '##e', ',', 'John', ',', 'Jim', ',', 'and', 'B', '##ob', '.']\n",
      "['I', \"'\", '##ve', 'check', '##e', '##d', ',', 'but', 'I', 'will', 'check', 'again', '.']\n"
     ]
    }
   ],
   "source": [
    "print(my_tokenizer_book.tokenize(\"There is still a lot of treasure buried on the island.\"))\n",
    "print(my_tokenizer_book.tokenize(\"I've checked, my last shipment was delayed.\"))\n",
    "print(my_tokenizer_book.tokenize(\"The captain and the lieutenant had a discussion.\"))\n",
    "print(my_tokenizer_book.tokenize(\"The team members are Alice, John, Jim, and Bob.\"))\n",
    "print(my_tokenizer_book.tokenize(\"I've checked, but I will check again.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb1854-955e-4c82-8ff9-df30cb0c3601",
   "metadata": {},
   "source": [
    "Assuming `max_vocab_size=20000`, we see that the token list contain \"full\" words we would expect given our training corpus being the book *Treasure Island*. For example, *\"John\"* and *\"Jim\"* are the names of characters in the book and therefore appear frequently. In contrast, the *\"Alice\"* and *\"Bob\"* never appear in the book and are as such Out-of-Vocabulary (OOV) tokens which are split into known tokens. The same is true for the two ranks *\"captain\"* (often appears in the book) and *\"lieutenant\"* (never appears in the book)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead5064-e817-4b71-9ff3-636467f4561c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6243c2-c6c2-4d65-ade5-04fc431dbae2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "WordPiece is a subword tokenization algorithm widely used in natural language processing (NLP) to handle the challenges of out-of-vocabulary (OOV) words and create efficient representations of text. Unlike word-level tokenization, which struggles with rare or novel words, and character-level tokenization, which loses semantic context, WordPiece breaks words into smaller, meaningful subword units. It begins with an initial vocabulary of characters and iteratively merges the most frequent token pairs in a corpus until a specified vocabulary size is reached. This approach ensures that both frequent and rare words are effectively represented, making WordPiece a cornerstone of modern NLP systems like BERT and its variants.\n",
    "\n",
    "WordPiece is crucial for enabling language models to generalize across diverse text inputs. By tokenizing words into known subunits, it allows models to process rare or unseen words without losing their semantic meaning. For example, a word like *\"unhappiness\"* might be split into *\"un\"*, *\"happy\"*, and \"*ness\"*, capturing its structure and meaning. WordPiece is especially effective in multilingual and morphologically rich languages, where it can create shared subword representations across languages. Its applications include machine translation, sentiment analysis, question answering, and more, where robust tokenization is essential for high model performance. Its main advantages are:\n",
    "\n",
    "* **OOV handling:** WordPiece effectively tokenizes rare or novel words into smaller, meaningful components, preventing issues with OOV words.\n",
    "* **Compact vocabulary:** The algorithm creates a vocabulary that balances granularity and efficiency, reducing memory and computational requirements.\n",
    "* **Multilingual support:** Shared subwords across languages make WordPiece well-suited for multilingual models like mBERT.\n",
    "* **Improved Generalization:** By focusing on statistically significant subword units, WordPiece supports better model performance across domains.\n",
    "\n",
    "However, WordPiece also has some limitations or potential problems, mainly:\n",
    "\n",
    "* **Computational overhead:** The iterative merging process during training can be computationally expensive for large corpora.\n",
    "* **Over-fragmentation:** Small vocabulary sizes can lead to overly fragmented tokenization, potentially obscuring word-level semantic meaning.\n",
    "* **Dependence on corpus:** WordPiece relies heavily on the training corpus, making the vocabulary less adaptable to new domains or specialized datasets.\n",
    "* **Interpretability:** Subword tokenization can result in less human-interpretable token sequences, particularly in text generation tasks.\n",
    "\n",
    "WordPiece strikes a balance between character- and word-level tokenization, offering an efficient and effective solution for modern NLP challenges. Despite its limitations, its ability to handle OOV words, create compact vocabularies, and support multilingual processing has made it indispensable in the development of state-of-the-art NLP models. However, careful tuning of vocabulary size and corpus selection is essential to maximize its benefits while mitigating its drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ce33d-2ceb-4f2f-ac7e-83b2fbf56647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
