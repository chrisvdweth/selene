{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9004612a-7de2-46a3-916a-dbc72ae2a258",
   "metadata": {},
   "source": [
    "<img src=\"images/logo/selene-logo-640.png\" style=\"max-height:75px;\" alt=\"SELENE Logo\" />\n",
    "\n",
    "**Disclaimer:** This Jupyter Notebook contains content generated with the assistance of AI. While every effort has been made to review and validate the outputs, users should independently verify critical information before relying on it. The SELENE notebook repository is constantly evolving. We recommend downloading or pulling the latest version of this notebook from Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cec76-721f-4227-ae55-14bae6046380",
   "metadata": {},
   "source": [
    "# Token Indexing with Vocabularies\n",
    "\n",
    "Token indexing using vocabularies in the context of machine learning for text is the process of mapping words, subwords, or characters in a text to unique numerical identifiers based on a predefined vocabulary. A vocabulary is a set of known tokens that a model can recognize, with each token assigned a corresponding index. For example, if a vocabulary consists of {hello: 1, world: 2, machine: 3, learning: 4}, then the phrase *\"hello world\"* would be represented as [1, 2]. This transformation allows text data to be processed by machine learning models, which operate on numerical inputs rather than raw text.\n",
    "\n",
    "Token indexing is essential because it provides a structured way to represent text, enabling machine learning models to process and analyze it efficiently. Without this step, words would have no standardized numerical representation, making it difficult for models to recognize patterns or relationships in language. Additionally, token indexing helps manage computational resources effectively by ensuring that text input is transformed into compact numerical sequences that can be fed into neural networks. Advanced tokenization methods, such as subword tokenization (e.g., Byte Pair Encoding or WordPiece), further enhance this process by breaking down rare or unknown words into smaller, more frequently occurring components.\n",
    "\n",
    "One of the most important aspects of token indexing is its connection to word embeddings. Once text is converted into token indices, these indices serve as inputs to an embedding layer in a neural network. The embedding layer maps each token index to a dense, high-dimensional vector that captures semantic and syntactic relationships between words. For instance, words with similar meanings, such as *\"king\"* and *\"queen\"*, would have embeddings that are closer in vector space. This transformation is crucial for deep learning models, as it allows them to understand language beyond simple numerical representations.\n",
    "\n",
    "By linking token indexing to word embeddings, machine learning models gain the ability to generalize and make sense of complex linguistic patterns. Pretrained embeddings, such as Word2Vec, GloVe, or contextual embeddings from Transformer models like BERT and GPT, enable models to leverage large amounts of text data to learn meaningful word associations. Ultimately, token indexing serves as the foundation for powerful NLP applications, including text classification, sentiment analysis, machine translation, and chatbots, making it a fundamental component of modern natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8663bc1-2835-4961-aad1-1233b938ae40",
   "metadata": {},
   "source": [
    "### Setting up the Notebook\n",
    "\n",
    "#### Make Required Imports\n",
    "\n",
    "This notebook requires the import of different Python packages but also additional Python modules that are part of the repository. If a package is missing, use your preferred package manager (e.g., [conda](https://anaconda.org/anaconda/conda) or [pip](https://pypi.org/project/pip/)) to install it. If the code cell below runs with any errors, all required packages and modules have successfully been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807f6d16-1d74-406b-a706-ff4763d2a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.libimports.textidx import *\n",
    "from src.utils.data.files import *\n",
    "from src.text.vectorizing.vocab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92c437-8bb6-44e7-8548-14a3026f080a",
   "metadata": {},
   "source": [
    "#### Download Required Data\n",
    "\n",
    "Some code examples in this notebook use data that first need to be downloaded by running the code cell below. If this code cell throws any error, please check the configuration file `config.yaml` if the URL for downloading datasets is up to date and matches the one on Github. If not, simply download or pull the latest version from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9cb480-5651-431f-820a-a649d93ad23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/datasets/text/classification/sentence-polarity/sentence-polarity.pos' already exists (use 'overwrite=True' to overwrite it).\n",
      "File 'data/datasets/text/classification/sentence-polarity/sentence-polarity.neg' already exists (use 'overwrite=True' to overwrite it).\n"
     ]
    }
   ],
   "source": [
    "sentences_pos, _ = download_dataset(\"text/classification/sentence-polarity/sentence-polarity.pos\")\n",
    "sentences_neg, _ = download_dataset(\"text/classification/sentence-polarity/sentence-polarity.neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5a21c-f8fc-4fed-8d7d-cd8557c4fedb",
   "metadata": {},
   "source": [
    "This notebook also generates new data and saves it as files into a specified output folder. You can change the default output folder in the code cell below to customize your file management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56152d70-0eb8-450f-a3e5-6a9fedf95b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = create_folder(\"data/generated/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd7793-ffd4-4911-bae9-c3f7d88bc18f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e8f72-4214-4008-a43f-171205868ae8",
   "metadata": {},
   "source": [
    "## Example Dataset\n",
    "\n",
    "We start with a very small example dataset and later look at (arguably still small) real-world dataset. This allows us to clearly see and understand all the involved steps and inspect the intermediate result after each step. For example, given the very small dataset size, we can actually print the complete vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3520a7-7712-4841-be0d-51692a28c64a",
   "metadata": {},
   "source": [
    "### Create Simple Classification Dataset\n",
    "\n",
    "The array `dataset_news` below represent a very simple classification dataset to train a model to predict whether a sentence from a news article should be labeled *\"politics\"* or *\"sports\"*. As we don't really train a model and follow each step, this dataset contains only 7 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dced5c14-9f6b-42c4-9e1f-d8e6b316364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_news = [\n",
    "    (\"The mayor was elected for this term and the next term.\", \"politics\"),\n",
    "    (\"A mayor's goal for the next term is to win.\", \"politics\"),\n",
    "    (\"The goal for this term was to win the vote.\", \"politics\"),\n",
    "    (\"This term's goals are next term's goals.\", \"politics\"),\n",
    "    (\"The goal of any team player is the win.\", \"sports\"),\n",
    "    (\"A win for the team is a win for each player.\", \"sports\"),\n",
    "    (\"Players vote other players for another term.\", \"sports\")\n",
    "]\n",
    "\n",
    "inputs_news = [ tup[0] for tup in dataset_news ]\n",
    "targets_news = [ tup[1] for tup in dataset_news ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed6f6e1-9ba6-40a4-b0ad-30ebeff72abd",
   "metadata": {},
   "source": [
    "### Prepare Class Labels\n",
    "\n",
    "Assuming that $C$ denotes the number of classes, most algorithms for training classification models in existing libraries &mdash; including neural network libraries such as PyTorch &mdash; assume that all classes are labeled from $0$ to $C-1$. Right now, our classes are labeled with the string *\"sports\"* and *\"politics\"*, so we need to convert those to adhere to the required format. Since we only have 2 classes for our example, we can do this \"manually\" by creating two dictionaries that map each string class label to a valid integer class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baad81d4-630f-405d-9702-e8766449828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2index_news = { \"politics\": 0, \"sports\": 1 }\n",
    "index2label_news = { 0: 'politics', 1: 'sports' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9bdf8-a24f-4cc3-af61-3a40f091b230",
   "metadata": {},
   "source": [
    "While this works just fine, we have to be careful to avoid mistakes, particularly when the number of classes increases. For one, we have to ensure that the mapping is consistent in both directions. For example, if *\"politics\"* maps to $0$, we have to make sure that $0$ maps back to *\"politics\"*; this must hold for all $C$ classes. We also have to make sure that we do not map two different class labels to the same index &mdash; although this would throw an error when trying to create the dictionary to map from indices back to the class labels since an index can only map to one label.\n",
    "\n",
    "To avoid these kinds of mistakes, we therefore do not create these two mappings manually but automatically. For this, let's first identify the unique set of labels for a given dataset, which is just two class labels for our very simple news dataset. We already have all labels for each data sample as a list. To get all unique labels, we can simply convert this list to a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfec9da-a6cd-4138-8f6e-1c0d30922d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sports', 'politics'}\n"
     ]
    }
   ],
   "source": [
    "labels_news = set(targets_news)\n",
    "\n",
    "print(labels_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c380f5-fe67-4a6d-91cb-9c88b4e0e958",
   "metadata": {},
   "source": [
    "To create a unique index from $0$ to $C-1$ for each class label, we can utilize the `enumerate()` method to make life easy. This is a built-in method in Python that adds a counter to an iterable (e.g., set, list, tuple, or dictionary) and returns it as an enumerate object, which can be directly converted into a list or iterated through in a loop. It is commonly used in for loops to simultaneously access both the index and the value of each item in the iterable. This method improves code readability and eliminates the need to manually maintain a counter when iterating over collections.\n",
    "\n",
    "The code cell below also uses the nice concept of dictionary comprehension in Python. Dictionary comprehensions provide a concise and elegant way to create dictionaries by combining loops and conditional logic in a single line of code. This approach is more readable and efficient compared to using explicit loops for dictionary creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cba7609-b892-43f2-8d77-18a7d5789972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sports': 0, 'politics': 1}\n"
     ]
    }
   ],
   "source": [
    "label2index_news = { label:index for index, label in enumerate(set(targets_news)) }\n",
    "\n",
    "print(label2index_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668db9c3-86b4-48e7-8984-6be9cba53971",
   "metadata": {},
   "source": [
    "Notice that this automated approach ensures that we do not accidentally map two different class labels to the same index. The only minor limitation is that we cannot specify which label gets mapped to which index, and vice versa. However, in practice, this basically never matters. It's only important that the indices are between `0` and `C-1` which is easily achieved. Of course, if the exact mapping does indeed matter, this can still easily be achieved, for example, by manually creating a list of all class labels with the desired order.\n",
    "\n",
    "We now can map from our string class labels to integer class labels, but we still miss the opposite direction that allows us to map back from the indices to the original class labels. To ensure that both mappings are consistent, we can create the mapping from indices to strings by \"reversing\" the `label2index_news` dictionary. Again, using dictionary comprehension this only takes a single line of code to accomplish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f7a94e-db69-4e91-9a77-8ac859b89edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sports', 1: 'politics'}\n"
     ]
    }
   ],
   "source": [
    "index2label_news = { v:k for k,v in label2index_news.items() }\n",
    "\n",
    "print(index2label_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe45c2-195b-4898-9d6d-869d45c28aa9",
   "metadata": {},
   "source": [
    "With these two dictionaries, we can now map back and forth between the original string labels and the index labels that serve as input for a classification algorithm. This to convert our list of string labels for all data samples to a vector of label indices, we can now use a list comprehension and the dictionary `label2index_news`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a2b72b-c075-4934-8786-f803a9dea111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politics', 'politics', 'politics', 'politics', 'sports', 'sports', 'sports'] ==> [1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "target_vector_news = [ label2index_news[label] for label in targets_news ]\n",
    "\n",
    "print(f\"{targets_news} ==> {target_vector_news}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a0d80-ee80-466c-817a-79fe89e17e14",
   "metadata": {},
   "source": [
    "Once a model is trained with these integer class labels, this model will output these integer class labels as its predictions. If we want to convert these integers back to the original string class labels, we can use the dictionary `index2label_news`. For example, if we assume that the model returns `1` as the prediction class label for some input text, we can simple get the string label using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf0a6d5-3aeb-43d0-a32e-3da5ecf66a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n"
     ]
    }
   ],
   "source": [
    "print(index2label_news[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6372022-a4df-4334-b8cb-bcb12dd0aeb8",
   "metadata": {},
   "source": [
    "With respect to the class labels we are done. The more interesting step is to convert our sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5ebf0-2432-463b-8fe7-4a1716dac646",
   "metadata": {},
   "source": [
    "### Create Vocabulary & Mappings\n",
    "\n",
    "Similar to the class labels, the overall goal is to map each unique word/token to a unique index, i.e., an integer identifier. And also similar to the labels, given a vocabulary size of `V` these unique indices must be of the range from $0$ to $V-1$. However, compared to the list of class labels we could directly convert to a set, for the inputs, we need to perform some additional steps; more specifically:\n",
    "\n",
    "* **Create vocabulary:** Like for the class labels, the vocabulary is simply the set of unique words &mdash; strictly speaking, unique tokens as the vocabulary may contain punctuation marks, numbers, or any other non-standard words. However, in practice, we also often want the number of occurrences of each token to potentially limit the vocabulary to the most common tokens\n",
    "\n",
    "* **Create mappings:** Once we have the vocabulary, we again need to map each token to a unique index, and vice versa. Of course, we again have to make sure that the mappings are consistent. We therefore create both mappings the same way we did for the class labels\n",
    "\n",
    "* **Additional considerations:** Converting between token and indices may involve various application-dependent considerations. The most common one is that we need to decide how to convert unseen tokens to a meaningful index. Not only do we typically limit the size of the vocabulary, but even if we would not, a new input text may always have tokens not present in the vocabulary.\n",
    "\n",
    "Well, let's do this..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e75de-c948-4320-90a0-2763df653eab",
   "metadata": {},
   "source": [
    "#### Create Vocabulary\n",
    "\n",
    "##### Compute Token Frequencies\n",
    "\n",
    "As already mentioned, we often want to filter out (very) tokens from the vocabulary. We therefore need to first compute the number of occurrences of all tokens/words in our corpus. Using the `Counter` class from Python's `collections` library makes this very easy. This is a specialized dictionary subclass designed to count the occurrences of elements in an iterable, such as a list or string. It stores elements as dictionary keys and their counts as values, making it easy to tally frequencies.\n",
    "\n",
    "**Important:** In this step, we also need to decide how we want to preprocess or filter the input text. For example, in the code cell below, we perform case-folding (i.e., converting all words to lowercase). In contrast, we do not perform stemming or lemmatization, and do keep all stopwords, punctuation marks, etc. Which preprocessing step are performed, strongly depends on the exact application context or other assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06812753-5be8-4d37-8946-40818eefc3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 8, 'term': 7, '.': 7, 'for': 6, 'win': 5, 'this': 3, 'next': 3, 'a': 3, \"'s\": 3, 'goal': 3, 'is': 3, 'mayor': 2, 'was': 2, 'to': 2, 'vote': 2, 'goals': 2, 'team': 2, 'player': 2, 'players': 2, 'elected': 1, 'and': 1, 'are': 1, 'of': 1, 'any': 1, 'each': 1, 'other': 1, 'another': 1})\n"
     ]
    }
   ],
   "source": [
    "# Auxiliary method to preprocess a text string\n",
    "def preprocess(text):\n",
    "    return [token.text.lower() for token in nlp(text) ]\n",
    "    \n",
    "\n",
    "# Create counter (a specialized dictionary)\n",
    "token_counter_news = Counter()\n",
    "\n",
    "for text in inputs_news:\n",
    "    for token in preprocess(text):\n",
    "        token_counter_news[token] += 1\n",
    "        \n",
    "print(token_counter_news)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de9dbe-c62d-42d4-9148-00b7851bac20",
   "metadata": {},
   "source": [
    "##### Sort Tokens by Frequency\n",
    "\n",
    "By default, the `Counter` class does not guarantee that the entries are sorted with respect to their counts. So let's create a sorted list of tuples, where each tuple contains the token and the number of occurrences. As usual, Python makes such things exceedingly easy using a single line of code using the `sorted()` method. This is a built-in method of Python that returns a new sorted list from the elements of any iterable, such as lists, tuples, or strings, without modifying the original iterable. The syntax is `sorted(iterable, key=None, reverse=False)`, where `key` is an optional parameter specifying a function to determine the sorting criteria, and `reverse` is a Boolean indicating whether to sort in descending order (default is `False` for ascending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142d5e76-34f0-426e-8ce5-05d3c3bddf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 27\n",
      "[('the', 8), ('term', 7), ('.', 7), ('for', 6), ('win', 5), ('this', 3), ('next', 3), ('a', 3), (\"'s\", 3), ('goal', 3), ('is', 3), ('mayor', 2), ('was', 2), ('to', 2), ('vote', 2), ('goals', 2), ('team', 2), ('player', 2), ('players', 2), ('elected', 1), ('and', 1), ('are', 1), ('of', 1), ('any', 1), ('each', 1), ('other', 1), ('another', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Sort by word frequency\n",
    "token_counter_news_sorted = sorted(token_counter_news.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(token_counter_news_sorted)))\n",
    "print(token_counter_news_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536e23f-c189-4ff8-a453-7a376fa83314",
   "metadata": {},
   "source": [
    "##### Limit Vocabulary\n",
    "\n",
    "Since we now have a sorted list, where the tokens are sorted with respect to their number of occurrences in an descending order, we can easily filter out only the *k* most frequent tokens. Since we only have 27 tokens in our \"full\" vocabulary, let's consider the top-25 most frequent tokens; see the code cell below. In practice, a \"full\" vocabulary can easily contain several hundred thousand tokens, with only top-20k to top-50k tokens considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f347468-bebb-4b22-b265-9532fbfeb839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 25\n",
      "[('the', 8), ('term', 7), ('.', 7), ('for', 6), ('win', 5), ('this', 3), ('next', 3), ('a', 3), (\"'s\", 3), ('goal', 3), ('is', 3), ('mayor', 2), ('was', 2), ('to', 2), ('vote', 2), ('goals', 2), ('team', 2), ('player', 2), ('players', 2), ('elected', 1), ('and', 1), ('are', 1), ('of', 1), ('any', 1), ('each', 1)]\n"
     ]
    }
   ],
   "source": [
    "TOP_TOKENS_NEWS = 25\n",
    "\n",
    "token_counter_news_sorted_filtered = token_counter_news_sorted[:TOP_TOKENS_NEWS]\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(token_counter_news_sorted_filtered)))\n",
    "print(token_counter_news_sorted_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c3f49-724a-4384-a0c1-9c054ed3977a",
   "metadata": {},
   "source": [
    "##### Create Final Vocabulary\n",
    "\n",
    "The get the final vocabulary, i.e., only the list or set of tokens we want to consider, we can use another list comprehension to extract all the tokens from the (token, count)-pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd09a5d-fe38-461f-861a-3254f1f4f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'term', '.', 'for', 'win', 'this', 'next', 'a', \"'s\", 'goal', 'is', 'mayor', 'was', 'to', 'vote', 'goals', 'team', 'player', 'players', 'elected', 'and', 'are', 'of', 'any', 'each']\n"
     ]
    }
   ],
   "source": [
    "tokens_news = [ tup[0] for tup in token_counter_news_sorted_filtered ]\n",
    "\n",
    "print(tokens_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968eb777-3014-4d4a-bf0a-06d35a7d7e4b",
   "metadata": {},
   "source": [
    "#### Create Mappings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f186ce4-3466-422a-a818-b7e86cd98bed",
   "metadata": {},
   "source": [
    "##### Special Tokens\n",
    "\n",
    "Many neural network models working with text require or benefit from special tokens beyond the tokens in the vocabulary derived from a training dataset. Special tokens such as `SOS` (start of Sequence), `EOS` (end of Sequence), `PAD` (padding), `UNK` (unknown), `SEP` (separator), and CLS (classification) play critical roles in preparing text data for neural networks, particularly in models for tasks like text generation, machine translation, and language understanding.\n",
    "\n",
    "* `SOS` and `EOS` mark the start and end of a sequence, helping models learn the boundaries of input or output text. These tokens guide sequence models in identifying where text begins and ends, which is crucial for tasks such as translation and text generation.\n",
    "* `PAD` tokens are used to standardize the lengths of sequences by padding shorter sequences to match the length of the longest one in a batch, facilitating efficient batch processing.\n",
    "* `UNK` represents out-of-vocabulary words not present in the training set, allowing models to handle unseen words during inference.\n",
    "\n",
    "In transformer-based models like BERT, `SEP` separates segments of text within a single input sequence, enabling the model to distinguish between different sentences or clauses. `CLS` is a special token placed at the beginning of the input and is used to aggregate the representation of the entire sequence for classification tasks. These special tokens help structure input text, improve processing efficiency, and enhance the ability to capture semantic information for various NLP tasks.\n",
    "\n",
    "These are the most common special tokens but others exists; which special tokens are indeed need depends on the exact model and task. For our example here, let's consider all the six special tokens we have just described by creating a list containing all special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c56db8a-f9ae-4d7c-9437-d85ba4120d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_PAD, TOKEN_UNK, TOKEN_SOS, TOKEN_EOS, TOKEN_SEP, TOKEN_CLS = \"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\", \"<SEP>\", \"<CLS>\"\n",
    "\n",
    "SPECIAL_TOKENS = [TOKEN_PAD, TOKEN_UNK, TOKEN_SOS, TOKEN_EOS, TOKEN_SEP, TOKEN_CLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417c87b-d1d9-4c76-8813-7fee6bfb2645",
   "metadata": {},
   "source": [
    "Note that the exact string representing the special tokens does not matter. It is only important to ensure, none of the special tokens is likely to already exist in our initial vocabulary derived from the training data. For this reason, we add angled brackets to each token since tokens such as *\"pad\"* or *\"sos\"* may already occur in the dataset. Still, instead of `<PAD>`, we could also use `[PAD]`, `<PADDING>`, `###PAD###`, and so on. In short, there is nothing intrinsically specific about the strings for the special tokens. However, our choices above adhere to best practices used in many implementations and educational materials.\n",
    "\n",
    "**Side note:** While not mandatory, we use `<PAD>` as the first special token in the list above. This ensures in a moment, that `<PAD>` will be mapped to the index $0$. This index is commonly assumed to be the index for padding tokens. However, it is generally not mandatory since one can specify explicitly which index indicates padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c0383-ddf7-4aaa-89d8-5f90904d952c",
   "metadata": {},
   "source": [
    "##### Token2Index & Index&Token Mappings\n",
    "\n",
    "With our initial vocabulary and the list of special tokens, we can create a mapping from tokens to unique indices the same way as we did for the class labels using a dictionary comprehension. Note that the dictionary comprehension iterates over the concatenation of the list of special tokens and the list of vocabulary tokens. We chose this order to ensure that the special tokens get index first, particularly so that `<PAD>` will get the index $0$. Again, this is not mandatory but a very common best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfbc7828-4fc8-43d4-8a74-ea805d2dc196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, '<SEP>': 4, '<CLS>': 5, 'the': 6, 'term': 7, '.': 8, 'for': 9, 'win': 10, 'this': 11, 'next': 12, 'a': 13, \"'s\": 14, 'goal': 15, 'is': 16, 'mayor': 17, 'was': 18, 'to': 19, 'vote': 20, 'goals': 21, 'team': 22, 'player': 23, 'players': 24, 'elected': 25, 'and': 26, 'are': 27, 'of': 28, 'any': 29, 'each': 30}\n"
     ]
    }
   ],
   "source": [
    "token2index_news = { token:index for index, token in enumerate(SPECIAL_TOKENS + tokens_news) }\n",
    "\n",
    "print(token2index_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a017b-b444-4e23-93a9-0b4a87e51ada",
   "metadata": {},
   "source": [
    "With this first dictionary mapping from tokens to indices, we can create the dictionary mapping the indices to tokens by reversing `token2index_news` using a dictionary comprehension. This ensures that both mappings are consistent using a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f68cdc3c-4ed3-47e2-9b99-8147cf6b6159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>', 4: '<SEP>', 5: '<CLS>', 6: 'the', 7: 'term', 8: '.', 9: 'for', 10: 'win', 11: 'this', 12: 'next', 13: 'a', 14: \"'s\", 15: 'goal', 16: 'is', 17: 'mayor', 18: 'was', 19: 'to', 20: 'vote', 21: 'goals', 22: 'team', 23: 'player', 24: 'players', 25: 'elected', 26: 'and', 27: 'are', 28: 'of', 29: 'any', 30: 'each'}\n"
     ]
    }
   ],
   "source": [
    "index2token_news = { v:k for k,v in token2index_news.items() }\n",
    "\n",
    "print(index2token_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e014bd-123d-4450-bc41-40de478f52d0",
   "metadata": {},
   "source": [
    "##### Additional Considerations: Handling Unknown Words\n",
    "\n",
    "With the dictionary `token2index_news` we can not map from tokens to their respective indices. However, this only works if the token is either a known special token or a token from the initial vocabulary. Otherwise, the token is not a valid key in our dictionary and trying to access it would result in an error. We therefore need to handle this case meaningfully. This is where the special token `<UNK>` comes into play. Any time we encounter a token that is not in our vocabulary, we treat it as \"unknown\" and map it to its respective index. We consider this the default index for unknown tokens, and we have to explicitly specify it for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d0ee876-82ba-4a0c-b958-4edd9a70803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "default_index = token2index_news[TOKEN_UNK]\n",
    "\n",
    "print(default_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613dbe8-922d-4736-963e-d53563cfdae6",
   "metadata": {},
   "source": [
    "#### Working with the Vocabulary & Mappings\n",
    "\n",
    "With the vocabulary and both mappings created, we can now use them for preparing our text data to serve as input for neural networks. For example, let's see which indices have been assigned to our special tokens. This task comes down to a simple lookup in the dictionary `token2index_news`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "115a0867-7501-4cfe-ac25-bfdb43d3331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of <PAD> in the vocabulary is: 0\n",
      "The index of <UNK> in the vocabulary is: 1\n",
      "The index of <SOS> in the vocabulary is: 2\n",
      "The index of <EOS> in the vocabulary is: 3\n",
      "The index of <SEP> in the vocabulary is: 4\n",
      "The index of <CLS> in the vocabulary is: 5\n"
     ]
    }
   ],
   "source": [
    "for special_token in SPECIAL_TOKENS:\n",
    "    token_index = token2index_news[special_token]\n",
    "    print(\"The index of {} in the vocabulary is: {}\".format(special_token, token_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfc905-40d7-40d1-ba8b-4e847c832c24",
   "metadata": {},
   "source": [
    "As you can see, the indices reflect the order of our special tokens in the list `SPECIAL_TOKENS`. And as mentioned above, having `<PAD>` mapped to $0$ is often convenient in practice. Of course, we can also map back from indices to the tokens. Let's do this for the first 10 indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d880a77-3c69-4e6a-b45d-90b022ed0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token at index 0: <PAD>\n",
      "Token at index 1: <UNK>\n",
      "Token at index 2: <SOS>\n",
      "Token at index 3: <EOS>\n",
      "Token at index 4: <SEP>\n",
      "Token at index 5: <CLS>\n",
      "Token at index 6: the\n",
      "Token at index 7: term\n",
      "Token at index 8: .\n",
      "Token at index 9: for\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Token at index {}: {}\".format(idx, index2token_news[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d2b89-5586-4e8f-aa75-249c869f9d34",
   "metadata": {},
   "source": [
    "In practice, we don't need to do this for each individual token or index. It's much more convenient to do the encoding for a complete list of tokens. Here, we have to address the case that a token might not have been in our vocabulary and therefore not a key in the dictionary `token2index_news`. Recall that we want to map to the index for the special token `UNK` (Unknown) in this case. For convenience and easy re-use, let's implement this using an auxiliary method `encode()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "003f10d0-a785-4bd5-8d13-85883c6c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens: list[str]):\n",
    "    return [ token2index_news[t] if t in token2index_news else default_index for t in tokens ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb49fa7-209a-445f-bfa9-c4f795b4b8f9",
   "metadata": {},
   "source": [
    "Let's first apply this method for a list of token where all tokens are in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c85a631a-c717-4fa0-b04b-94d7b13b2910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 17, 18, 25]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(['the', 'mayor', 'was', 'elected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96823e-441b-4670-a21d-90b546ec626e",
   "metadata": {},
   "source": [
    "The code cell below shows an example for what happens if the input list contains a previously unseen token (here: *president*). As required, unseen tokens get replaced by the index representing our special tokens `<UNK>` which is `1` in our case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e2860fc-37cd-4f5f-8680-5984254fac98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 1, 18, 25]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(['the', 'president', 'was', 'elected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676a74b-b396-471b-94cd-5c2b0261626a",
   "metadata": {},
   "source": [
    "Similarly, we also want a method that decodes a list of indices back to their respective tokens; the method `decode()` below accomplishes this. Notice this method also takes a `default_token` as an input parameter in case of an unknown index. When decoding the output of a neural network, this case should never happen, as the network will never return an unknown index. Still, users can call this method manually with arbitrary indices and we should handle this case gracefully without throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90a4f240-3b44-44d0-9799-18004d2ec8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(indices: list[int], default_token=\"<???>\"):\n",
    "    return [ index2token_news[i] if i in index2token_news else default_token for i in indices ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b8c26-9596-402d-bf5d-93fd9ab2e852",
   "metadata": {},
   "source": [
    "The code cell below shows the basic use of this method; here, all indices are mapped to existing tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5dafe39-d983-43f0-b1b0-361a9a804dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'mayor', 'was', 'elected', '<EOS>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([6, 17, 18, 25, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05cc9b-ae81-4e6d-bcf2-2482e9a38f13",
   "metadata": {},
   "source": [
    "Of course, if the list of indices contains the index that is not known, it maps to the specified `default_token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c93785e-e005-4adb-b20e-52b390c179f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', '<???>', 'was', 'elected', '<EOS>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([6, 99, 18, 25, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0902a8-88dd-4916-af2e-5a16ea92a6fd",
   "metadata": {},
   "source": [
    "#### Vectorize Corpus\n",
    "\n",
    "Finally, we can now vectorize our text documents (i.e., our news article sentences) by simply applying the `encode()` method to each sentence in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b50df3e4-fb33-4080-8a5d-79d13d9700b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 17, 18, 25, 9, 11, 7, 26, 6, 12, 7, 8],\n",
       " [13, 17, 14, 15, 9, 6, 12, 7, 16, 19, 10, 8],\n",
       " [6, 15, 9, 11, 7, 18, 19, 10, 6, 20, 8],\n",
       " [11, 7, 14, 21, 27, 12, 7, 14, 21, 8],\n",
       " [6, 15, 28, 29, 22, 23, 16, 6, 10, 8],\n",
       " [13, 10, 9, 6, 22, 16, 13, 10, 9, 30, 23, 8],\n",
       " [24, 20, 1, 24, 9, 1, 7, 8]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vectors_news = [ encode(preprocess(text)) for text in inputs_news ]\n",
    "\n",
    "input_vectors_news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e139ee3-9cdc-4a4e-8c35-659bc3a9d8d2",
   "metadata": {},
   "source": [
    "**Important:** While now each sentence is now a list of indices (e.g., integer values) and thus strictly speaking a vector, the indices are still just labels and carry no semantic meaning. As such, for example, it would not make sense to compute the vector similarity between those vectors (even when assuming both vectors have the same length). However, this is the default representation for text input. Word semantics are added using a word embedding layer (covered) in a later notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9951042-f5fe-4a40-9d13-f833fd8e0636",
   "metadata": {},
   "source": [
    "### Practical Application\n",
    "\n",
    "#### Class Implementation\n",
    "\n",
    "These basic steps of (a) creating the vocabulary, (b) creating the mappings between tokens and indices, and (c) any additional consideration (e.g., default index) are very common and almost always exactly the same. It is therefore very useful consolidate all these steps into their own class for easy re-use. We did so by implementing a class `Vocabulary` which you can find in the file `src.text.vectorizing.vocab.py`. In its core, this class implements the methods all the methods we have seen so far. So let's use our example datset of news article sentences to see how we can use this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a89c738-8e9f-4e44-a48a-2a14f295e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_news = Vocabulary(tokens_news, SPECIAL_TOKENS)\n",
    "\n",
    "vocabulary_news.set_default_index(vocabulary_news[TOKEN_UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71c23a-77ae-4465-94a5-5d2fe8b92771",
   "metadata": {},
   "source": [
    "As this class implements the `encode()` method we have seen before, we can use it to encode a list of input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f836e8a0-6148-4a43-b823-63850b262371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  1 18 25]\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_news.encode(['the', 'president', 'was', 'elected']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa436a-8c50-4da8-89e2-7e785a87db70",
   "metadata": {},
   "source": [
    "Conversely, we can use the class method `decode()` to map a list of indices back to their respective tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8faa07ba-1534-4d8c-b756-b0c9ea21bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'mayor', 'was', 'elected', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_news.decode([6, 17, 18, 25, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506bccf-72b1-4efb-86ed-f6364a2c1643",
   "metadata": {},
   "source": [
    "If you think about it out initial set of string class label is also just a vocabulary; for our news dataset only containing the two tokens *\"sports\"* and *\"politics\"*. As such, we can also use the `Vocabulary` class not only for the tokens but also for the class labels. With this, we can directly benefit from the `encode()` and `decode()` method provided by the class without required additional code for handling the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ced877cd-8133-49bf-b40f-fde016e9b825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "['politics', 'sports', 'politics']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_news_targets = Vocabulary(labels_news)\n",
    "\n",
    "print(vocabulary_news_targets.encode([\"sports\", \"sports\", \"politics\"]))\n",
    "\n",
    "print(vocabulary_news_targets.decode([1, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c65091-b4b2-4d19-af1c-9f2bc76f01f4",
   "metadata": {},
   "source": [
    "#### Save Vectorized Dataset & Vocabularies\n",
    "\n",
    "In practice, we often deal with very large datasets. This means that creating the vocabulary and vectorizing the corpus can take a significant amount of time &mdash; note this also includes any potentially time-consuming preprocessing. It is therefore common to consider this as an individual step and save the vectorized dataset to be used for training later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a769fd-78b0-44a2-9aa1-e849f7eb3ae4",
   "metadata": {},
   "source": [
    "##### Save Inputs & Targets\n",
    "\n",
    "In the code cell below, we loop through all sentences in our toy corpus, vectorize a sentence and save the resulting sequence together with the class label directly to a file as a new line. This has the advantage that there is no need to keep the whole vectorized dataset in memory.\n",
    "\n",
    "**Side note:** In the code cells below, we use a naming scheme to reflect the number of tokens in the vocabulary (excluding the special tokens). Such naming schemes can be useful when the same raw input data gets converted into different datasets using different preprocessing steps of vocabulary settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f91ce6f-5934-46fb-9112-1b87d09c5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(f\"{output_folder}toy-news-dataset-vectors-{TOP_TOKENS_NEWS}.txt\", \"w\")\n",
    "\n",
    "for idx, text in enumerate(inputs_news):\n",
    "    # Get label\n",
    "    label = vocabulary_news_targets.encode([targets_news[idx]])[0]\n",
    "    # Get sentence and vectorize it using the vocabulary\n",
    "    vector = vocabulary_news.encode(preprocess(text))\n",
    "    # Write sequence and label to file (separate sequence and label using a tab)\n",
    "    output_file.write(f\"{' '.join([str(idx) for idx in vector])}\\t{label}\\n\")\n",
    "        \n",
    "output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd7edc-0960-4c11-93fe-26f00006591f",
   "metadata": {},
   "source": [
    "##### Save Vocabularies\n",
    "\n",
    "We also need to save the two vocabularies to save the mappings between the tokens/labels and their indices. Without it, we would only have a dataset of integer sequences without knowing which tokens or class labels those integers represent. We can still train a model &mdash; after all, this is why we vectorize the dataset to begin with &mdash; however, then we could not predict the class labels for new sentences since we would now know how to vectorize those new sentences. For this, we can simply use the `pickle` library for Python. This library is used for serializing and deserializing Python objects, meaning it converts Python objects into a byte stream (serialization) and reconstructs them back into their original form (deserialization). This is useful for saving objects to a file, sending data over a network, or storing complex data structures for later use. The pickle module supports a wide range of Python data types, including lists, dictionaries, and even custom objects.\n",
    "\n",
    "The `dump()` method in the `pickle` library takes two main arguments: the object to be serialized and the file where the serialized data should be stored. The syntax is `pickle.dump(obj, file, protocol=None)`, where `protocol` specifies the serialization format (defaulting to the latest protocol available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0a72f5d-a545-4934-ac1a-a0d424d0c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_folder}toy-news-dataset-{TOP_TOKENS_NEWS}.vocab\", 'wb') as out_file:\n",
    "    pickle.dump(vocabulary_news, out_file)\n",
    "\n",
    "with open(f\"{output_folder}/toy-news-dataset-targets-{TOP_TOKENS_NEWS}.vocab\", 'wb') as out_file:\n",
    "    pickle.dump(vocabulary_news_targets, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63025985-dc3e-4955-b5ee-ac8ee50dffd1",
   "metadata": {},
   "source": [
    "Of course, once saved, we can load both vocabularies using the corresponding `load()` method. The syntax is `pickle.load(file)`, where `file` is the binary file containing the serialized data. This method reads the stored byte stream and converts it back into the original Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c587d9ad-d2ef-4cd3-af83-3ac90867b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_folder}toy-news-dataset-{TOP_TOKENS_NEWS}.vocab\", \"rb\") as in_file:\n",
    "    vocabulary_news = pickle.load(in_file)\n",
    "\n",
    "with open(f\"{output_folder}/toy-news-dataset-targets-{TOP_TOKENS_NEWS}.vocab\", \"rb\") as in_file:\n",
    "    vocabulary_news_targets = pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03497129-d4b4-4a99-8533-83e53b969796",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312cf71-be9e-4101-b802-1366664e708f",
   "metadata": {},
   "source": [
    "## Real-World Dataset\n",
    "\n",
    "The Sentence Polarity Dataset is a popular benchmark for binary text classification tasks, particularly for sentiment analysis. It consists of a total of 10,662 positive and negative sentence samples extracted from movie reviews. The dataset contains two classes: one for positive sentiment and another for negative sentiment, with no neutral examples. Each sentence is labeled according to its sentiment polarity &mdash; either positive or negative. This dataset is commonly used for training and evaluating machine learning models to understand and classify the sentiment expressed in short text snippets. It serves as a foundation for various NLP applications, including sentiment analysis tools, recommendation systems, and opinion mining. The binary classification task aims to correctly predict whether a given sentence conveys a positive or negative sentiment based solely on its textual content.\n",
    "\n",
    "The sentences are split across two files according to their sentiment. As such, the sentiment labels can be derived from the file names, more specifically, from their extensions `.pos` and `.neg`. We have downloaded both files at the beginning of the notebook and have both file names stored in the variables `sentences_pos` and `sentences_neg`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcc807-d3e8-4ead-94f6-0607e4f96f6f",
   "metadata": {},
   "source": [
    "### Read Files & Compute Word Frequencies\n",
    "\n",
    "The first step is again to go through the whole corpus and count the number of occurrences for each token. For really large corpora this can actually take quite some time. 10k sentences is basically nothing these days, but the purpose of this notebook is not to focus on large scale data as the steps would be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0e7be3f-c34c-4dc3-9ee3-c245910eb86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10662/10662 [00:53<00:00, 199.75it/s]\n"
     ]
    }
   ],
   "source": [
    "token_counter_polarity = Counter()\n",
    "\n",
    "targets_polarity = []\n",
    "\n",
    "with tqdm(total=10662) as pbar:\n",
    "    # Loop over all file names\n",
    "    for file_name in [sentences_pos, sentences_neg]:\n",
    "        # Get sentiment label from file name extensions\n",
    "        label = file_name.split(\".\")[-1]\n",
    "        # Loop over each sentence (1 sentence per line)\n",
    "        with open(file_name) as file:\n",
    "            for line in file:\n",
    "                # Update token counts\n",
    "                for token in preprocess(line):\n",
    "                    token_counter_polarity[token] += 1            \n",
    "                # Add label to targets list\n",
    "                targets_polarity.append(label)\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "# Identify set of unique class labels\n",
    "labels_polarity = set(targets_polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e075f-f4df-46c0-a41b-57b62cf05127",
   "metadata": {},
   "source": [
    "### Prepare Class Labels\n",
    "\n",
    "When preprocessing the sentences, we also create a list of class labels for each sentence. We can use this list of labels as input for the `Vocabulary` class to get our mapping from class labels to class indices and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee82fb48-6b55-488b-9a57-f411c3c4d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'pos': 1}\n",
      "{0: 'neg', 1: 'pos'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary_polarity_targets = Vocabulary(labels_polarity)\n",
    "\n",
    "print(vocabulary_polarity_targets.token2index)\n",
    "print(vocabulary_polarity_targets.index2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f4806-ec6f-4601-95cc-95f4b6fc6d0c",
   "metadata": {},
   "source": [
    "### Create Vocabulary\n",
    "\n",
    "To create our `vocab` object, we perform exactly the same steps as above. The only difference is that our \"full\" vocabulary is now larger (although with less than 20k tokens still rather small). We therefore limit the vocabulary here to the 10,000 most frequent tokens. We also combine steps of sorting and filtering the tokens in the same code cell; we already saw what the individually steps are doing for our simple news article dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7ba958a-204b-46a8-8baf-39711c233be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by token frequency\n",
    "token_counter_polarity_sorted = sorted(token_counter_polarity.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Limit number of tokens to the top-10000 most frequent tokens\n",
    "TOP_TOKENS_POLARITY = 10000\n",
    "token_counter_polarity_sorted_filtered = token_counter_polarity_sorted[:TOP_TOKENS_POLARITY]\n",
    "\n",
    "# Extract final list of tokens\n",
    "tokens_polarity = [ tup[0] for tup in token_counter_polarity_sorted_filtered ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4bad41-e23e-482d-8b5d-df3ad5d888b0",
   "metadata": {},
   "source": [
    "With `tokens_polarity` containing all the tokens we want to capture in our vocabulary, we create a `Vocabulary` instance using this list. To keep it simple and consistent, we also include the same list of special tokens as before. We also should not forget to set the default index or appropriately handle unknown tokens when encoding an input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f840adc9-1a3d-4154-b65e-bbdae59c1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_polarity = Vocabulary(tokens_polarity, special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "vocabulary_polarity.set_default_index(vocabulary_polarity[TOKEN_UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c829efb-388a-40c4-8668-b52b9e3c8d96",
   "metadata": {},
   "source": [
    "For illustration, we can use this vocabulary to encode an example sentence to its corresponding list of token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0775e4a8-2be0-45d8-b5ba-2dddfc11461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   8   26  106   34   19   62    9   21   50  490    8  257 2554    6]\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_polarity.encode([\"the\", \"movie\", \"was\", \"not\", \"that\", \"good\", \",\", \"but\", \"i\", \"left\", \"the\", \"cinema\", \"entertained\", \".\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0eae2c-4e42-4a05-953a-8de2eec2a1f7",
   "metadata": {},
   "source": [
    "### Save Dataset & Vocabularies\n",
    "\n",
    "Lastly, for later use, we can again save all the data to files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54197206-2a8a-4ee1-a7c3-69d8702c89cd",
   "metadata": {},
   "source": [
    "#### Vectorize and Save Dataset\n",
    "\n",
    "Like before, we use the vocabulary to vectorize all sentences &mdash; convert all sentences to their corresponding list of token indices &mdash; and save all vectorized sentences, together with the transformed class label, to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7192c849-ef39-43cd-9078-6a53c512920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10662/10662 [00:50<00:00, 213.08it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file = open(f\"{output_folder}polarity-dataset-vectors-{TOP_TOKENS_POLARITY}.txt\", \"w\")\n",
    "\n",
    "with tqdm(total=10662) as pbar:\n",
    "    for file_name in [sentences_pos, sentences_neg]:\n",
    "        # Get class label from file name    \n",
    "        label_name = file_name.split(\".\")[-1]\n",
    "        # Iterate over all sentences, vectorize and save them\n",
    "        with open(file_name) as file:\n",
    "            for line in file:\n",
    "                label = vocabulary_polarity_targets.encode([label_name])[0]\n",
    "                vector = vocabulary_polarity.encode(preprocess(line))\n",
    "                output_file.write(f\"{' '.join([str(idx) for idx in vector])}\\t{label}\\n\")\n",
    "                pbar.update(1)\n",
    "            \n",
    "output_file.flush()\n",
    "output_file.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2a371-1779-402e-a0a7-fdbce7697e6f",
   "metadata": {},
   "source": [
    "#### Save Vocabularies\n",
    "\n",
    "We need to remember both mappings between the tokens and their indices, as well as between the class labels and their indices. The easiest way to to do this is once again to simply save both `Vocabulary` instances using the `pickle` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1af89ff5-4a2f-4ed8-b744-8480e1e497ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_folder}polarity-dataset-{TOP_TOKENS_POLARITY}.vocab\", \"wb\") as out_file:\n",
    "    pickle.dump(vocabulary_polarity, out_file)\n",
    "\n",
    "with open(f\"{output_folder}polarity-dataset-targets-{TOP_TOKENS_POLARITY}.vocab\", 'wb') as out_file:\n",
    "    pickle.dump(vocabulary_polarity_targets, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e35b9-23fb-413d-ae96-9c2356c44ac7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b2a69-4eea-4c61-a4ad-8a060aca3c55",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In machine learning, particularly in natural language processing (NLP), converting text into sequences of token IDs based on a predefined vocabulary is essential for training and deploying models. Since machine learning algorithms work with numerical data, raw text must be transformed into a numerical representation that models can process effectively. Tokenization, followed by mapping tokens to unique numerical IDs, enables this conversion while preserving the structure and meaning of the text.\n",
    "\n",
    "A vocabulary serves as a reference that assigns a unique integer ID to each token (word, subword, or character) found in the dataset. This process helps standardize input data and ensures consistency in how text is represented across different models and tasks. Using token IDs instead of raw words reduces memory consumption and computational complexity, allowing models to handle large-scale text data more efficiently. Additionally, assigning token IDs enables the use of embedding layers, where words with similar meanings can be mapped to nearby points in a continuous vector space, improving model performance.\n",
    "\n",
    "The importance of converting text into token sequences extends to various NLP applications, such as text classification, machine translation, and sentiment analysis. Pretrained language models like BERT and GPT use subword tokenization techniques (e.g., WordPiece or Byte Pair Encoding) to handle rare and out-of-vocabulary words, ensuring that even unknown words are decomposed into meaningful subunits. This enhances model generalization and allows it to process diverse linguistic patterns more effectively.\n",
    "\n",
    "Moreover, tokenization and vocabulary-based encoding play a critical role in sequence-based models like recurrent neural networks (RNNs) and transformers. These models rely on structured numerical input to learn contextual relationships between words. The use of token IDs also enables batching, padding, and attention mechanisms, which are crucial for efficient training and inference. Without proper text-to-token conversion, NLP models would struggle to learn meaningful representations, leading to suboptimal performance.\n",
    "\n",
    "In summary, converting text into token sequences using a vocabulary is a foundational step in NLP and machine learning. It bridges the gap between human language and numerical computation, facilitating effective model training and deployment. By standardizing text representation, improving efficiency, and enabling better generalization, this process ensures that machine learning models can process and understand language in a structured and meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f7535-e528-4fed-ba58-c4413a008317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
